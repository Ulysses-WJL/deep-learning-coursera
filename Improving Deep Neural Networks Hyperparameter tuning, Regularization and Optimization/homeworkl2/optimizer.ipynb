{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_input_dims, n_neurons, activation=None, weights=None, bias=None):\n",
    "        self.weights = weights if weights is not None else np.random.randn(\n",
    "            n_input_dims, n_neurons) * np.sqrt(2 / n_input_dims)\n",
    "        self.bias = bias if bias is not None else np.zeros(n_neurons)\n",
    "        self.activation = activation\n",
    "        self.A = None\n",
    "        self.dA = None\n",
    "        self.dZ = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.vdW = np.zeros_like(self.weights)\n",
    "        self.vdb = np.zeros_like(self.bias)\n",
    "        self.sdW = np.zeros_like(self.weights)\n",
    "        self.sdb = np.zeros_like(self.bias)\n",
    "\n",
    "    def activate(self, X):\n",
    "        Z = np.dot(X, self.weights) + self.bias\n",
    "        # 激活函数的输出\n",
    "        self.A = self._apply_activation(Z)\n",
    "        return self.A\n",
    "    \n",
    "    def _apply_activation(self, Z):\n",
    "        # 激活函数\n",
    "        if self.activation is None:\n",
    "            return Z\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0, Z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-Z))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(Z)\n",
    "        else:\n",
    "            return Z\n",
    "    \n",
    "    def derivative(self, A):\n",
    "        # 激活函数的导数\n",
    "        if self.activation is None:\n",
    "            return np.ones_like(A)\n",
    "        elif self.activation == 'relu':\n",
    "            # return np.where(A <= 0, 0., 1.)\n",
    "            d = np.array(A, copy=True)\n",
    "            d[A <= 0] = 0.\n",
    "            d[A > 0] = 1.\n",
    "            return d \n",
    "        elif self.activation == 'sigmoid':\n",
    "            return A  * (1 - A)\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - A ** 2\n",
    "        else:\n",
    "            return A\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"weigths: {self.weights.shape}\\n bias: {self.bias.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    def __init__(self, lambda_=1e-3, lr=1e-3, max_epochs=10000, \n",
    "                 batch_size=64, optimizer='adam', beta1=0.9,\n",
    "                 beta2=0.999, epsilon=1e-8, verbose=1, random_state=0):\n",
    "        self.layers = []\n",
    "        self.lr = lr\n",
    "        self.lambda_ = lambda_\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.verbose = verbose\n",
    "        self.epsilon = epsilon\n",
    "        self.seed = random_state\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.activate(X)\n",
    "        return X\n",
    "\n",
    "    def loss(self, y_true, y_preds):\n",
    "        # 交叉熵误差\n",
    "        assert(y_true.shape == y_preds.shape)\n",
    "        term = 0\n",
    "        for layer in self.layers:\n",
    "            term += np.sum(layer.weights ** 2)\n",
    "        loss_ = - y_true* np.log(y_preds + self.epsilon) - (1 - y_true)*np.log(1 - y_preds + self.epsilon)  \n",
    "        loss_ =  np.mean(loss_, axis=0) + self.lambda_ * term\n",
    "\n",
    "        return np.squeeze(loss_)\n",
    "    \n",
    "    def backpropagation(self, X, y, t):\n",
    "        # 反向传播算法 计算每一层的dW, db\n",
    "        # 前向计算 得到输出值\n",
    "        m = X.shape[0]  # batch size\n",
    "        self.feed_forward(X)  # (m, 1)\n",
    "        # print(out.shape)\n",
    "        # loss_ = self.loss(y, out)\n",
    "        for i in reversed(range(len(self.layers))):  # 从最后一层开始\n",
    "            layer = self.layers[i]\n",
    "            o_i = X if i == 0 else self.layers[i-1].A\n",
    "            if layer == self.layers[-1]:  # 输出层\n",
    "                # 使用 交叉熵 误差\n",
    "                assert(y.shape == layer.A.shape)\n",
    "                layer.dA = -y/(layer.A + self.epsilon) + (1-y) / (1-layer.A + self.epsilon)\n",
    "                # layer.dA = -y / out + (1-y)/(1-out)  # dL/dA^K  # (m, 1)\n",
    "            else:  # 隐藏层\n",
    "                next_layer = self.layers[i + 1]\n",
    "                layer.dA = next_layer.dZ @ next_layer.weights.T  # dL/dA^J (m, 1) (1, k)\n",
    "                # dL/dZ^J (m, k) (m,k)\n",
    "            layer.dZ = layer.dA * layer.derivative(layer.A)\n",
    "            layer.dW =  o_i.T @ layer.dZ * 1 / m\n",
    "            layer.db = np.sum(layer.dZ) * 1 / m\n",
    "            w_term = self.lambda_ * layer.weights * 1 / m\n",
    "\n",
    "            if self.optimizer == 'Momentum':\n",
    "                layer.vdW = self.beta1 * layer.vdW + (1 - self.beta1) * layer.dW\n",
    "                layer.vdb = self.beta1 * layer.vdb + (1 - self.beta1) * layer.db\n",
    "                layer.weights -= self.lr * (layer.vdW + w_term)\n",
    "                layer.bias -= self.lr * layer.vdb\n",
    "            elif self.optimizer == 'RMSprop':\n",
    "                layer.sdW = self.beta2 * layer.sdW + (1 - self.beta2) * layer.dW ** 2\n",
    "                layer.sdb = self.beta2 * layer.sdb + (1 - self.beta2) * layer.db ** 2  \n",
    "                layer.weights -= self.lr * (layer.dW / np.sqrt(layer.sdW + self.epsilon) + w_term)\n",
    "                layer.bias -= self.lr * layer.db / np.sqrt(layer.sdb + self.epsilon)\n",
    "            elif self.optimizer == 'Adam':\n",
    "                vdW = self.beta1 * layer.vdW + (1 - self.beta1) * layer.dW\n",
    "                vdb = self.beta1 * layer.vdb + (1 - self.beta1) * layer.db\n",
    "                layer.vdW = vdW / (1 - self.beta1 ** t)\n",
    "                layer.vdb = vdb / (1 - self.beta1 ** t)\n",
    "                # assert(np.all(layer.vdW != np.inf))\n",
    "                sdW = self.beta2 * layer.sdW + (1 - self.beta2) * layer.dW ** 2\n",
    "                sdb = self.beta2 * layer.sdb + (1 - self.beta2) * layer.db ** 2\n",
    "                layer.sdW = sdW / (1 - np.power(self.beta2, t))\n",
    "                layer.sdb = sdb / (1 -  np.power(self.beta2, t))\n",
    "                # print(layer.sdW)\n",
    "                # assert(np.all(layer.sdW != np.inf))\n",
    "                layer.weights -= self.lr * layer.vdW / np.sqrt(layer.sdW + self.epsilon) + self.lr * w_term\n",
    "                layer.bias -= self.lr * layer.vdb / np.sqrt(layer.sdb + self.epsilon)                       \n",
    "            else:\n",
    "                layer.weights -= self.lr * (layer.dW + w_term)\n",
    "                layer.bias -=  self.lr * layer.db\n",
    "         \n",
    "    def preprocessing(self, X, y):\n",
    "        y = y.reshape(-1, 1)\n",
    "        self.seed += 1\n",
    "        np.random.seed(self.seed)\n",
    "        permutation = np.random.permutation(X.shape[0])\n",
    "        shuffled_X = X[permutation]\n",
    "        shuffled_y = y[permutation]\n",
    "        \n",
    "        return shuffled_X, shuffled_y\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # X, y = self.preprocessing(X, y)\n",
    "        cross_entropy = []\n",
    "        # accuracy = []\n",
    "        batch_size = self.batch_size\n",
    "        m = X.shape[0]\n",
    "        batches = m // batch_size if m % batch_size == 0 else m // batch_size + 1\n",
    "        \n",
    "        for epoch in range(self.max_epochs):\n",
    "            # 每个epoch X,y 顺序都不一样\n",
    "            t = 0\n",
    "            shuffled_X, shuffled_y = self.preprocessing(X, y)\n",
    "            # print(self.seed)\n",
    "            for step in range(batches):\n",
    "                \n",
    "                batch_X = shuffled_X[batch_size*step: batch_size*(step+1)]\n",
    "                batch_y = shuffled_y[batch_size*step: batch_size*(step+1)]\n",
    "                # t 需要从1开始\n",
    "                t += 1\n",
    "                self.backpropagation(batch_X, batch_y, t)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.loss(y, self.feed_forward(X))\n",
    "                cross_entropy.append(loss)\n",
    "            if epoch % 1000 == 0 and self.verbose:\n",
    "#                 predict = self.predict(X_train)\n",
    "#                 train_acc = np.sum(y_train.ravel()==predict) / len(predict)\n",
    "                \n",
    "#                 predict = self.predict(X_test)\n",
    "#                 acc = np.sum(y_test.ravel()==predict) / len(predict)\n",
    "#                 accuracy.append(acc)\n",
    "                print(f'Epoch: {epoch}, cross_entropy: {loss}')\n",
    "        print(t)\n",
    "        return cross_entropy\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.feed_forward(X)\n",
    "        y_pred = np.where(y_pred >=0.5, 1, 0)\n",
    "        # out = np.argmax(y_pred, axis=1)\n",
    "        return np.squeeze(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision(model, X, y):\n",
    "    x_max = X[:, 0].max() + 1\n",
    "    x_min = X[:, 0].min() - 1\n",
    "    y_max = X[:, 1].max() + 1\n",
    "    y_min = X[:, 1].min() - 1\n",
    "    xx, yy = np.mgrid[x_min:x_max:0.01, y_min:y_max:0.01]\n",
    "    zz = np.c_[xx.ravel(), yy.ravel()]\n",
    "    preds = model.predict(zz)\n",
    "    plt.contourf(xx, yy, preds.reshape(xx.shape), cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=np.squeeze(y), cmap=plt.cm.Spectral)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
