{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_moons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = make_moons(300, noise=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_input_dims, n_neurons, activation=None, weights=None, bias=None, random_state=0):\n",
    "        np.random.seed(random_state)\n",
    "        self.weights = weights if weights is not None else np.random.randn(\n",
    "            n_input_dims, n_neurons) * np.sqrt(2 / n_input_dims)\n",
    "        self.bias = bias if bias is not None else np.zeros(n_neurons)\n",
    "        self.activation = activation\n",
    "        self.A = None\n",
    "        self.dA = None\n",
    "        self.dZ = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.vdW = np.zeros_like(self.weights)\n",
    "        self.vdb = np.zeros_like(self.bias)\n",
    "        self.sdW = np.zeros_like(self.weights)\n",
    "        self.sdb = np.zeros_like(self.bias)\n",
    "        # 用来记录梯度的平方\n",
    "        self.dW_2 =  np.zeros_like(self.weights)\n",
    "        self.db_2 = np.zeros_like(self.bias)\n",
    "\n",
    "    def activate(self, X):\n",
    "        Z = np.dot(X, self.weights) + self.bias\n",
    "        # 激活函数的输出\n",
    "        self.A = self._apply_activation(Z)\n",
    "        return self.A\n",
    "    \n",
    "    def _apply_activation(self, Z):\n",
    "        # 激活函数\n",
    "        if self.activation is None:\n",
    "            return Z\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0, Z)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-Z))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(Z)\n",
    "        else:\n",
    "            return Z\n",
    "    \n",
    "    def derivative(self, A):\n",
    "        # 激活函数的导数\n",
    "        if self.activation is None:\n",
    "            return np.ones_like(A)\n",
    "        elif self.activation == 'relu':\n",
    "            # return np.where(A <= 0, 0., 1.)\n",
    "            d = np.array(A, copy=True)\n",
    "            d[A <= 0] = 0.\n",
    "            d[A > 0] = 1.\n",
    "            return d \n",
    "        elif self.activation == 'sigmoid':\n",
    "            return A  * (1 - A)\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - A ** 2\n",
    "        else:\n",
    "            return A\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"weigths: {self.weights.shape}\\n bias: {self.bias.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gradient descent with Momentum\n",
    "\n",
    "Requrie: 全局学习率 $\\alpha$, 动量参数$\\beta$  \n",
    "Requrie: 初始参数 $\\theta$, 初始速度$v=0$  \n",
    "while 没有达到停止准则 do  \n",
    "    \n",
    "- 计算当前mini-batch的梯度: $g$   \n",
    "- 计算速度更新: $v \\leftarrow \\beta v + (1-\\beta)g$  \n",
    "- 应用更新: $\\theta \\leftarrow \\theta - \\alpha v$  \n",
    "\n",
    "end while\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  AdaGrad\n",
    "\n",
    "Requrie: 全局学习率 $\\alpha$  \n",
    "Requrie: 初始参数 $\\theta$  \n",
    "Require: 小常数$\\epsilon, 通常设为10^{-8}$(用来被小数除时的数值稳定)  \n",
    "初始化梯度累计变量$r = 0$  \n",
    "while 没有达到停止准则 do  \n",
    "\n",
    "- 计算当前mini-batch的梯度: $g$   \n",
    "- 累计平方梯度: $r \\leftarrow  r + g\\odot g$  \n",
    "- 应用更新: $\\theta \\leftarrow \\theta - \\alpha \\frac{1}{\\sqrt{r}+\\epsilon}\\odot g$  \n",
    "\n",
    "end while\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RMSProp\n",
    "\n",
    "Requrie: 全局学习率 $\\alpha$, 衰减速率$\\beta$    \n",
    "Requrie: 初始参数 $\\theta$  \n",
    "Require: 小常数$\\epsilon, 通常设为10^{-8}$(用来被小数除时的数值稳定)  \n",
    "初始化累计变量$r = 0$  \n",
    "while 没有达到停止准则 do  \n",
    "\n",
    "- 计算当前mini-batch的梯度: $g$   \n",
    "- 累计平方梯度: $r \\leftarrow \\beta r + (1-\\beta)g\\odot g$  \n",
    "- 应用更新: $\\theta \\leftarrow \\theta - \\alpha \\frac{g}{\\sqrt{r}}$  \n",
    "\n",
    "end while\n",
    "\n",
    "\n",
    "$\\beta$最常用的值是0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Adam\n",
    "\n",
    "Requrie: 全局学习率 $\\alpha$  \n",
    "Requrie: 矩估计指数衰减速率$\\beta_1, \\beta_2 \\in [0, 1)$(建议默认为0.9和0.999)    \n",
    "Requrie: 初始参数 $\\theta$  \n",
    "Require: 小常数$\\epsilon, 通常设为10^{-8}$(用来被小数除时的数值稳定)  \n",
    "初始化参数$\\theta$  \n",
    "初始化一阶和二阶矩变量$s=0, r=0$\n",
    "初始化同步时间$t=0$  \n",
    "while 没有达到停止准则 do  \n",
    "\n",
    "- 计算当前mini-batch的梯度: $g$ \n",
    "- $t \\leftarrow t + 1$\n",
    "- 更新有偏一阶矩估计: $s \\leftarrow \\beta_1 s + (1-\\beta_1)g$\n",
    "- 更新有偏二阶矩估计: $s \\leftarrow \\beta_2 r + (1-\\beta_2)g \\odot g$\n",
    "- 修正一阶矩的偏差: $\\hat s \\leftarrow \\frac{s}{1-\\beta_1^t}$\n",
    "- 修正一阶矩的偏差: $\\hat r \\leftarrow \\frac{r}{1-\\beta_2^t}$\n",
    "- 应用更新: $\\theta \\leftarrow \\theta - \\alpha \\frac{\\hat s}{\\sqrt{\\hat r} + \\epsilon}$  \n",
    "\n",
    "end while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    def __init__(self, lambda_=1e-3, lr=1e-3, max_epochs=10000, \n",
    "                 batch_size=64, optimizer='Adam', beta1=0.9,\n",
    "                 beta2=0.999, epsilon=1e-8, verbose=1, random_state=0):\n",
    "        self.layers = []\n",
    "        self.lr = lr\n",
    "        self.lambda_ = lambda_\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.verbose = verbose\n",
    "        self.epsilon = epsilon\n",
    "        self.seed = random_state\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.activate(X)\n",
    "        return X\n",
    "\n",
    "    def loss(self, y_true, y_preds):\n",
    "        # 交叉熵误差\n",
    "        assert(y_true.shape == y_preds.shape)\n",
    "        term = 0\n",
    "        for layer in self.layers:\n",
    "            term += np.sum(layer.weights ** 2)\n",
    "        loss_ = - y_true* np.log(y_preds + self.epsilon) - (1 - y_true)*np.log(1 - y_preds + self.epsilon)  \n",
    "        loss_ =  np.mean(loss_, axis=0) + self.lambda_ * term\n",
    "\n",
    "        return np.squeeze(loss_)\n",
    "    \n",
    "    def backpropagation(self, X, y, t):\n",
    "        # 反向传播算法 计算每一层的dW, db\n",
    "        # 前向计算 得到输出值\n",
    "        m = X.shape[0]  # batch size\n",
    "        self.feed_forward(X)  # (m, 1)\n",
    "        # print(out.shape)\n",
    "        # loss_ = self.loss(y, out)\n",
    "        for i in reversed(range(len(self.layers))):  # 从最后一层开始\n",
    "            layer = self.layers[i]\n",
    "            o_i = X if i == 0 else self.layers[i-1].A\n",
    "            if layer == self.layers[-1]:  # 输出层\n",
    "                # 使用 交叉熵 误差\n",
    "                assert(y.shape == layer.A.shape)\n",
    "                layer.dA = -y/(layer.A + self.epsilon) + (1-y) / (1-layer.A + self.epsilon)\n",
    "                # layer.dA = -y / out + (1-y)/(1-out)  # dL/dA^K  # (m, 1)\n",
    "            else:  # 隐藏层\n",
    "                next_layer = self.layers[i + 1]\n",
    "                layer.dA = next_layer.dZ @ next_layer.weights.T  # dL/dA^J (m, 1) (1, k)\n",
    "                # dL/dZ^J (m, k) (m,k)\n",
    "            layer.dZ = layer.dA * layer.derivative(layer.A)\n",
    "            layer.dW =  o_i.T @ layer.dZ * 1 / m\n",
    "            layer.db = np.sum(layer.dZ) * 1 / m\n",
    "            w_term = self.lambda_ * layer.weights * 1 / m\n",
    "\n",
    "            if self.optimizer == 'Momentum':\n",
    "                layer.vdW = self.beta1 * layer.vdW + (1 - self.beta1) * layer.dW\n",
    "                layer.vdb = self.beta1 * layer.vdb + (1 - self.beta1) * layer.db\n",
    "                layer.weights -= self.lr * (layer.vdW + w_term)\n",
    "                layer.bias -= self.lr * layer.vdb\n",
    "            elif self.optimizer == 'RMSprop':\n",
    "                layer.sdW = self.beta2 * layer.sdW + (1 - self.beta2) * layer.dW ** 2\n",
    "                layer.sdb = self.beta2 * layer.sdb + (1 - self.beta2) * layer.db ** 2  \n",
    "                layer.weights -= self.lr * (layer.dW / (layer.sdW ** 0.5 + self.epsilon) + w_term)\n",
    "                layer.bias -= self.lr * layer.db / (layer.sdb ** 0.5 + self.epsilon)\n",
    "            elif self.optimizer == 'AdaGrad':\n",
    "                layer.dW_2 = layer.dW_2 + layer.dW ** 2\n",
    "                layer.db_2 = layer.db_2 + layer.db ** 2\n",
    "                layer.weights -= self.lr * (layer.dW / (layer.dW_2 ** 0.5 + self.epsilon) + w_term)\n",
    "                layer.bias -= self.lr * layer.db / (layer.db_2 ** 0.5 + self.epsilon)\n",
    "            elif self.optimizer == 'Adam':\n",
    "                layer.vdW = self.beta1 * layer.vdW + (1 - self.beta1) * layer.dW\n",
    "                layer.vdb = self.beta1 * layer.vdb + (1 - self.beta1) * layer.db\n",
    "                # layer.vdW = vdW / (1 - self.beta1 ** t)\n",
    "                # layer.vdb = vdb / (1 - self.beta1 ** t)\n",
    "    \n",
    "                layer.sdW = self.beta2 * layer.sdW + (1 - self.beta2) * layer.dW ** 2\n",
    "                layer.sdb = self.beta2 * layer.sdb + (1 - self.beta2) * layer.db ** 2\n",
    "                # layer.sdW = sdW / (1 - np.power(self.beta2, t))\n",
    "                # layer.sdb = sdb / (1 -  np.power(self.beta2, t))\n",
    "\n",
    "                # 改变计算顺序而得到提升效率 防止溢出\n",
    "                lr = self.lr * (1 - self.beta2 ** t) ** 0.5 / (1 - self.beta1 ** t)\n",
    "                layer.weights -= lr * layer.vdW / (layer.sdW ** 0.5 + self.epsilon) + self.lr * w_term\n",
    "                layer.bias -= lr * layer.vdb / (layer.sdb ** 0.5 + self.epsilon)                       \n",
    "            else:\n",
    "                layer.weights -= self.lr * (layer.dW + w_term)\n",
    "                layer.bias -=  self.lr * layer.db\n",
    "         \n",
    "    def preprocessing(self, X, y):\n",
    "        y = y.reshape((-1, 1))\n",
    "        self.seed += 1\n",
    "        np.random.seed(self.seed)\n",
    "        permutation = np.random.permutation(X.shape[0])\n",
    "        shuffled_X = X[permutation]\n",
    "        shuffled_y = y[permutation]\n",
    "        \n",
    "        return shuffled_X, shuffled_y\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y = y.reshape(-1, 1)\n",
    "        cross_entropy = []\n",
    "        # accuracy = []\n",
    "        batch_size = self.batch_size\n",
    "        m = X.shape[0]\n",
    "        batches = m // batch_size if m % batch_size == 0 else m // batch_size + 1\n",
    "        t = 0\n",
    "        for epoch in range(self.max_epochs):\n",
    "            # 每个epoch X,y 顺序都不一样\n",
    "            \n",
    "            shuffled_X, shuffled_y = self.preprocessing(X, y)\n",
    "            # print(self.seed)\n",
    "            for step in range(batches):\n",
    "                \n",
    "                batch_X = shuffled_X[batch_size*step: batch_size*(step+1)]\n",
    "                batch_y = shuffled_y[batch_size*step: batch_size*(step+1)]\n",
    "                # t 需要从1开始\n",
    "                t += 1\n",
    "                self.backpropagation(batch_X, batch_y, t)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.loss(y, self.feed_forward(X))\n",
    "                cross_entropy.append(loss)\n",
    "            if epoch % 1000 == 0 and self.verbose:\n",
    "#                 predict = self.predict(X_train)\n",
    "#                 train_acc = np.sum(y_train.ravel()==predict) / len(predict)\n",
    "                \n",
    "#                 predict = self.predict(X_test)\n",
    "#                 acc = np.sum(y_test.ravel()==predict) / len(predict)\n",
    "#                 accuracy.append(acc)\n",
    "                print(f'Epoch: {epoch}, cross_entropy: {loss}')\n",
    "        return cross_entropy\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.feed_forward(X)\n",
    "        y_pred = np.where(y_pred >=0.5, 1, 0)\n",
    "        # out = np.argmax(y_pred, axis=1)\n",
    "        return np.squeeze(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision(model, X, y):\n",
    "    x_max = X[:, 0].max() + 1\n",
    "    x_min = X[:, 0].min() - 1\n",
    "    y_max = X[:, 1].max() + 1\n",
    "    y_min = X[:, 1].min() - 1\n",
    "    xx, yy = np.mgrid[x_min:x_max:0.01, y_min:y_max:0.01]\n",
    "    zz = np.c_[xx.ravel(), yy.ravel()]\n",
    "    preds = model.predict(zz)\n",
    "    plt.contourf(xx, yy, preds.reshape(xx.shape), cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=np.squeeze(y), cmap=plt.cm.Spectral)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = DeepNeuralNetwork(lambda_=0, lr=0.0007, max_epochs=10000, optimizer='gd')\n",
    "model_1.add_layer(Layer(X_train.shape[1], 5, activation='relu'))\n",
    "model_1.add_layer(Layer(5, 2, activation='relu'))\n",
    "model_1.add_layer(Layer(2, 1, activation='sigmoid'))\n",
    "\n",
    "cross_entropy = model_1.fit(X_train, y_train)\n",
    "preds = model_1.predict(X_train)\n",
    "np.mean(preds == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cross_entropy)\n",
    "plt.show()\n",
    "plt.title(\"Model with Gradient descent\")\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5, 2.5])\n",
    "axes.set_ylim([-1, 1.5])\n",
    "plot_decision(model_1, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch with Momentum mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = DeepNeuralNetwork(lambda_=0, lr=0.0007, max_epochs=10000, optimizer='Momentum')\n",
    "model_2.add_layer(Layer(X_train.shape[1], 5, activation='relu'))\n",
    "model_2.add_layer(Layer(5, 2, activation='relu'))\n",
    "model_2.add_layer(Layer(2, 1, activation='sigmoid'))\n",
    "\n",
    "cross_entropy = model_2.fit(X_train, y_train)\n",
    "preds = model_2.predict(X_train)\n",
    "np.mean(preds == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cross_entropy)\n",
    "plt.show()\n",
    "plt.title(\"Model with Momentum optimization\")\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5, 2.5])\n",
    "axes.set_ylim([-1, 1.5])\n",
    "plot_decision(model_2, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch with RMSprop mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = DeepNeuralNetwork(lambda_=0, lr=0.0007, max_epochs=10000, optimizer='RMSprop')\n",
    "model_3.add_layer(Layer(X_train.shape[1], 5, activation='relu'))\n",
    "model_3.add_layer(Layer(5, 2, activation='relu'))\n",
    "model_3.add_layer(Layer(2, 1, activation='sigmoid'))\n",
    "\n",
    "cross_entropy = model_3.fit(X_train, y_train)\n",
    "preds = model_3.predict(X_train)\n",
    "np.mean(preds == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cross_entropy)\n",
    "plt.show()\n",
    "plt.title(\"Model with RMSprop optimization\")\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5, 2.5])\n",
    "axes.set_ylim([-1, 1.5])\n",
    "plot_decision(model_3, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch with Adam mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1\n",
    "(1 - 0.999 ** t) ** 0.5 / (1 - 0.9 ** t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = DeepNeuralNetwork(lambda_=0, lr=0.0007, max_epochs=10000, optimizer='Adam')\n",
    "model_4.add_layer(Layer(X_train.shape[1], 5, activation='relu'))\n",
    "model_4.add_layer(Layer(5, 2, activation='relu'))\n",
    "model_4.add_layer(Layer(2, 1, activation='sigmoid'))\n",
    "\n",
    "cross_entropy = model_4.fit(X_train, y_train)\n",
    "preds = model_4.predict(X_train)\n",
    "np.mean(preds == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cross_entropy)\n",
    "plt.show()\n",
    "plt.title(\"Model with Adam optimization\")\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5, 2.5])\n",
    "axes.set_ylim([-1, 1.5])\n",
    "plot_decision(model_4, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = DeepNeuralNetwork(lambda_=0, lr=0.001, max_epochs=10000, optimizer='AdaGrad')\n",
    "model_5.add_layer(Layer(X_train.shape[1], 5, activation='relu'))\n",
    "model_5.add_layer(Layer(5, 2, activation='relu'))\n",
    "model_5.add_layer(Layer(2, 1, activation='sigmoid'))\n",
    "\n",
    "cross_entropy = model_5.fit(X_train, y_train)\n",
    "preds = model_5.predict(X_train)\n",
    "np.mean(preds == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cross_entropy)\n",
    "plt.show()\n",
    "plt.title(\"Model with AdaGrad optimization\")\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5, 2.5])\n",
    "axes.set_ylim([-1, 1.5])\n",
    "plot_decision(model_5, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 权重衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network2(DeepNeuralNetwork):\n",
    "    def __init__(self, lambda_=0.001, lr=0.001, max_epochs=10000, batch_size=64, optimizer='Adam', beta1=0.9,\n",
    "                 beta2=0.999, epsilon=1e-08, verbose=1, random_state=0, decay_rate=0.5):\n",
    "        super().__init__(lambda_=lambda_, lr=lr, max_epochs=max_epochs, batch_size=batch_size, optimizer=optimizer,\n",
    "                         beta1=beta1, beta2=beta2, epsilon=epsilon, verbose=verbose, random_state=random_state)\n",
    "        self.decay_rate = decay_rate\n",
    "    def fit(self, X, y):\n",
    "        y = y.reshape(-1, 1)\n",
    "        cross_entropy = []\n",
    "        # accuracy = []\n",
    "        batch_size = self.batch_size\n",
    "        m = X.shape[0]\n",
    "        batches = m // batch_size if m % batch_size == 0 else m // batch_size + 1\n",
    "        t = 0\n",
    "        lr = self.lr\n",
    "        for epoch in range(self.max_epochs):\n",
    "            # 每个epoch X,y 顺序都不一样\n",
    "            \n",
    "            self.lr = lr * (1 / (1 + epoch * self.decay_rate))\n",
    "            shuffled_X, shuffled_y = self.preprocessing(X, y)\n",
    "            # print(self.seed)\n",
    "            for step in range(batches):\n",
    "                \n",
    "                batch_X = shuffled_X[batch_size*step: batch_size*(step+1)]\n",
    "                batch_y = shuffled_y[batch_size*step: batch_size*(step+1)]\n",
    "                # t 需要从1开始\n",
    "                t += 1\n",
    "                self.backpropagation(batch_X, batch_y, t)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.loss(y, self.feed_forward(X))\n",
    "                cross_entropy.append(loss)\n",
    "            if epoch % 1000 == 0 and self.verbose:\n",
    "#                 predict = self.predict(X_train)\n",
    "#                 train_acc = np.sum(y_train.ravel()==predict) / len(predict)\n",
    "                \n",
    "#                 predict = self.predict(X_test)\n",
    "#                 acc = np.sum(y_test.ravel()==predict) / len(predict)\n",
    "#                 accuracy.append(acc)          \n",
    "                print(f'Epoch: {epoch}, cross_entropy: {loss}')\n",
    "        return cross_entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = Network2(lambda_=0, lr=0.0007, max_epochs=10000, optimizer='gd', decay_rate=0.0001)\n",
    "model_5.add_layer(Layer(X_train.shape[1], 5, activation='relu'))\n",
    "model_5.add_layer(Layer(5, 2, activation='relu'))\n",
    "model_5.add_layer(Layer(2, 1, activation='sigmoid'))\n",
    "\n",
    "cross_entropy = model_5.fit(X_train, y_train)\n",
    "preds = model_5.predict(X_train)\n",
    "np.mean(preds == y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
