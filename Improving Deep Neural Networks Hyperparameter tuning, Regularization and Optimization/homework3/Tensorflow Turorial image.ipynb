{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['figure.titlesize'] = 20\n",
    "plt.rcParams['figure.figsize'] = [9, 7]\n",
    "plt.rcParams['font.family'] = ['SimHei']# ['Noto Sans CJK JP']\n",
    "plt.rcParams['axes.unicode_minus']=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test_orig[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of X_train: ', X_train_orig.shape)\n",
    "print('Shape of y_train: ', Y_train_orig.shape)\n",
    "print('Shape of X_test: ', X_test_orig.shape)\n",
    "print('Shape of y_test: ', Y_test_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X, y):\n",
    "    # X:　(None, 64, 64, 3)\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255.\n",
    "    X = tf.reshape(X, (-1, 64*64*3))\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y_onehot = tf.one_hot(y, depth=6)\n",
    "    return X, y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices((X_train_orig, Y_train_orig))\n",
    "test_db = tf.data.Dataset.from_tensor_slices((X_test_orig, Y_test_orig))\n",
    "train_db = train_db.shuffle(2000).batch(64).map(preprocessing)\n",
    "test_db = test_db.shuffle(2000).batch(64).map(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracy = []\n",
    "max_epoch = 800\n",
    "lr = 1e-3\n",
    "\n",
    "# 初始化 参数\n",
    "w1, b1 = tf.Variable(tf.random.normal([64*64*3, 25], stddev=0.1)), tf.Variable(tf.zeros([25]))\n",
    "w2, b2 = tf.Variable(tf.random.normal([25, 12], stddev=0.1)), tf.Variable(tf.zeros([12]))\n",
    "w3, b3 = tf.Variable(tf.random.normal([12, 6], stddev=0.1)), tf.Variable(tf.zeros([6]))\n",
    "\n",
    "acc_meter = tf.keras.metrics.Accuracy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "st = time.process_time()\n",
    "for epoch in range(max_epoch):\n",
    "    cost = 0\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            h1 = x @ w1 + b1\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            h2 = h1 @ w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            out = h2 @ w3 + b3\n",
    "            # out = tf.nn.softmax(out)\n",
    "            loss = tf.keras.losses.categorical_crossentropy(y, out, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            cost += loss\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        optimizer.apply_gradients(zip(grads, [w1, b1, w2, b2, w3, b3]))\n",
    "#         for p, g in zip([w1, b1, w2, b2, w3, b3], grads):\n",
    "#             p.assign_sub(lr * g)\n",
    "    cost = cost / (step + 1)\n",
    "            \n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, 'loss', float(cost))\n",
    "        losses.append(cost)\n",
    "\n",
    "        total_correct, nums = 0., X_test_orig.shape[0]\n",
    "        for x, y in test_db:\n",
    "            out = tf.nn.relu(tf.nn.relu(x@w1 + b1) @ w2 + b2) @ w3 + b3\n",
    "            # h1 = x @ w1 + b1\n",
    "            # h1 = tf.nn.relu(h1)\n",
    "            # h2 = h1 @ w2 + b2\n",
    "            # h2 = tf.nn.relu(h2)\n",
    "            # out = h2 @ w3 + b3\n",
    "            # 预测的标签\n",
    "            y_pred = tf.argmax(out, axis=1)\n",
    "            # 实际标签\n",
    "            y_ = tf.argmax(y, axis=1)\n",
    "            # 正确的\n",
    "            acc_meter.update_state(y_, y_pred)\n",
    "#             correct = tf.cast(y_pred == y_, dtype=tf.int32)\n",
    "#             total_correct += tf.reduce_sum(correct).numpy()\n",
    "        print(epoch, 'Evaluate Acc:', float(acc_meter.result()))\n",
    "        accuracy.append(float(acc_meter.result()))\n",
    "        acc_meter.reset_states()\n",
    "\n",
    "end = time.process_time()\n",
    "print('耗时:', end-st)\n",
    "x = [i*100 for i in range(len(losses))]\n",
    "plt.figure()\n",
    "plt.plot(x, losses, color='C0', marker='s', label='训练误差')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('train.svg')\n",
    "\n",
    "x = [i*100 for i in range(len(accuracy))]\n",
    "plt.figure()\n",
    "plt.plot(x, accuracy, color='C1', marker='s', label='测试集准确率')\n",
    "plt.ylabel('准确率')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('test.svg')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = r'datasets/fingers/2.png'\n",
    "my_image = plt.imread(image_file)\n",
    "out = tf.nn.relu(tf.nn.relu(my_image.reshape(1, -1)@w1 + b1) @ w2 + b2) @ w3 + b3\n",
    "print(\"预测值: \", np.argmax(out))\n",
    "plt.imshow(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = r'datasets/fingers/1.png'\n",
    "my_image = plt.imread(image_file)\n",
    "out = tf.nn.relu(tf.nn.relu(my_image.reshape(1, -1)@w1 + b1) @ w2 + b2) @ w3 + b3\n",
    "print(\"预测值: \", np.argmax(out))\n",
    "plt.imshow(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = r'datasets/fingers/3.png'\n",
    "my_image = plt.imread(image_file)\n",
    "out = tf.nn.relu(tf.nn.relu(my_image.reshape(1, -1)@w1 + b1) @ w2 + b2) @ w3 + b3\n",
    "print(\"预测值: \", np.argmax(out))\n",
    "plt.imshow(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = r'datasets/fingers/4.png'\n",
    "my_image = plt.imread(image_file)\n",
    "out = tf.nn.relu(tf.nn.relu(my_image.reshape(1, -1)@w1 + b1) @ w2 + b2) @ w3 + b3\n",
    "print(\"预测值: \", np.argmax(out))\n",
    "plt.imshow(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = r'datasets/fingers/5.png'\n",
    "my_image = plt.imread(image_file)\n",
    "out = tf.nn.relu(tf.nn.relu(my_image.reshape(1, -1)@w1 + b1) @ w2 + b2) @ w3 + b3\n",
    "print(\"预测值: \", np.argmax(out))\n",
    "plt.imshow(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
