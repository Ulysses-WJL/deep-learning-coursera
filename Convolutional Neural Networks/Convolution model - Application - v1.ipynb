{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Application\n",
    "\n",
    "Welcome to Course 4's second assignment! In this notebook, you will:\n",
    "\n",
    "- Implement helper functions that you will use when implementing a TensorFlow model\n",
    "- Implement a fully functioning ConvNet using TensorFlow \n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "\n",
    "- Build and train a ConvNet in TensorFlow for a classification problem \n",
    "\n",
    "We assume here that you are already familiar with TensorFlow. If you are not, please refer the *TensorFlow Tutorial* of the third week of Course 2 (\"*Improving deep neural networks*\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - TensorFlow model\n",
    "\n",
    "In the previous assignment, you built helper functions using numpy to understand the mechanics behind convolutional neural networks. Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call. \n",
    "\n",
    "As usual, we will start by loading in the packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from homework1.cnn_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to load the \"SIGNS\" dataset you are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('homework1/datasets/train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "    test_dataset = h5py.File('homework1/datasets/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the data (signs)\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train :  (1080, 64, 64, 3) (1080,)\nTest :  (120, 64, 64, 3) (120,)\n"
    }
   ],
   "source": [
    "print('Train : ', X_train_orig.shape, Y_train_orig.shape)\n",
    "print('Test : ', X_test_orig.shape, Y_tes_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5.\n",
    "\n",
    "<img src=\"images/SIGNS.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of `index` below and re-run to see different examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "y = 2\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"250.618594pt\" version=\"1.1\" viewBox=\"0 0 251.565 250.618594\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 250.618594 \nL 251.565 250.618594 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 226.740469 \nL 244.365 226.740469 \nL 244.365 9.300469 \nL 26.925 9.300469 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pc1b2e4927c)\">\n    <image height=\"218\" id=\"imaged2cf45264a\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAABHNCSVQICAgIfAhkiAAAIABJREFUeJztXVmTHMdxru7puWf2wuIGCIIASZEUD0iESFGiRCnssB1h/0G/OcJ+cyhk67BsybQoiaIuSpQEEhQlHoCIiyCOvWZn5+j2A6iuL7MnEzWjZT3l91SzWV1V0z21nZn1ZWby39/818KJSMpWUdBuRZGX7Tz37WlO++HnvOBTwfjkzynrlYKIygroi8MXRUL64bL4OqbTfGa7KvPXTVi/yWRatsfjMZGNxyDDfpOJuEbHblXhEkFIvyf5xO5Vmnppmvh2ktAxKpMLosnUf8/tjduk26V3flO2Nz/6kMhqzt+7Zt3P3croetv1GsgyImuBrN2sE1kDvmcd7kG9TsevpdI9pfcRu9WzGumH93TEnnsOz5fObDAYPhXYRjMYIiBzFbUBIGsozhHVTHkFJ9iLDkJUPfx7zlWXKQi5CutlqM0VylwV1RHmC1YdQQV0jqqHI6YSjsf+82Qiq6m4Ln6vEnojZzXvAVQlLpOftKwqFjld42i0V7bv3rpatq++d4H02717s2zXUzpGA/69o7rYaTD1ENQ0VBWdoypcPeX3yrenuX8utQldR1KDhRRUhqYRqtm8H8rGE646whjOYDB86rCNZjBEgG00gyECMsU57Aon2wwFuGiJzZDwEcGlzGwBYuWAfcKPEnJNhvYVce+TbsxGozIyRs7d9rNtqgnT98eB7n2083I2F9qVaSpbVMQ65vcbPqbqs5CPbqZTb1MOB9tE9vG198v2R5ff9tcMt0g/tMPqbB31mv/cgDa3tdAbz98IKdhKxZQ9UHIPfDvn/cDPkPAh0CyDv09yapsjRsxux+drbzSDIQJsoxkMEZBRjZCfjiNzg7vtZ6shrsb1smJW8951xB2PY7O5UO1j6yCfiIopLqOqVjpU5/jxwewx+f2gn9j/rwTVbFDHOQNGYWuQ+632QxYNlaXC85xMRqTfztbdsn398h+I7O61d/2Hqb+uQb3vDjRCxwgZhPHRBpd+xr4LGYP9Jmr4zJjKVgO3/RQeGv/t4P1JmWlEVXdQUxXSDGdF7Y2NGWIwRIVtNIMhAmyjGQwRkKEuWtU/FXoWYYb7C9OC2wyKXx2uQ2YLG8IJ7KPK+MTG4f3IrNwGDLON0KRKC5nFnTI7tZZDX5gr5z5lMi9j3qONliZiP2m9zlE61WTsqVRbG7dIv6sfvFW2716/RMfPwe6AW8VOO1wKXzmr0XvVBBsKmfZNxt5HO6zGbTR8TjX6zPB4gkZ0KAaWghwjVRhtLoUvymVI/7I3msEQAbbRDIYIyFCp4mQE6jrXggNl3a4aVIhdwW2KamSFTS7Q/NkfCJOFqzKzR7v3mTBb2BoFt3qRMlUGtKNpztXK2cGvfI14nFJxzYOqXoN2yoI7awojfTz16uLe4E7Z/vDd35N+N698AENQ1zmyezJYY5v58JugUnXrlJXfA/d+E9o1fiqCz7PC9MHAY7pGZPCMxrPVSOeqzxqRCypnhVU09XPzqBN81vZGMxgiwDaawRABGScLy5CZEIXcjTFgmSiZrbYWle0vMDyVlXC1AB1CaS7rlQWfXPBUcfUWNcka8zoWOeQ1gb8nTHckpF+mEmYY6Fj3OTIaLCCyBh64fLJHZLc2PePj3Td/VbZvXP0z6YfeM04Ex5QZXVAXVzsN0m+57T93m1R1rBNPt58r4Q9XIZNjMO24Eqzr1Tl81DzgF6/KmWkkBSXzMUZjnIv9rkx1NBjiwjaawRABttEMhgggyjPTMIn+zJO0IMuduForejbYDJXxYTzFhYo5GjXmPWFFsI7UHuT/X6AvzxtZzA7e48l/aoQZL7NLavDluH2Mrnm0w5xzrgks9zq4yxkpwk3Gu2X7g0vvEdlvfvGTsn3ntmeDFMzGQRZGt0Hvx3qvBe1m2e4zGw0tRx6ASn4JcD94wiO0ecYT2YU/ZrQUjO8cgWjC7ncOv4PccYbN7HdQJTAYxpzmNClTMTVmiMEQFbbRDIYIyPD1XFHtMKdh5fQd3Zqz1Ujn5BP2e59BrQxVHZkagtPhzDwlOHHXMk1GSxcu5TysqENADUlrGes9W13kLnzMpZFPWbpwUAlHY/9N94Y7pN9bv3u9bP/p7beIbLg78CuC515j33G169XAI0stIuu1vEqLZJBpJb05sjrYUQjm9CBHK3JOTTUXp6NHHHswaNHulO3llTXSb+XAgbLd7fWJrN7wajEyT3jekc3NzbJ99epVIrt106vn9kYzGCLANprBEAG20QyGCMhQ/+TmySTURlP65UrZpmK2l9dV0u8ROkxYTv1qP5miT8ZghiqxP1GQMIY+2FdZpVySt9lwam5fvXfhjbJ9+Y8XiWx34O0rXCI+B+ec2xXsMOdYEk9C8qf99vZ80p2NHUY52vO0rkZNZt63kTLGEvdkQhArpzChK519TTeCk6l0aZXITp46XbbPPvpY2T589Bjp1wL7LavR4wl8UCRBE3MgbO/4Z3jpMqWyvXnhTb9GZzAYPnXYRjMYIiCjahNjN6PrXy1nNLv6Jx+zknoa1UVUMbn6pgXeCappteLn7Pwk9+YGlsGYuqkn8BnVYp6GGiuRtjs9ImuCijLY9C7fd37zY9Lv9g1fGXN3j6YVJyWpCmS50O/ZAjWtzpItZsiEgGqdkwn9fzsGPW1rhz1PHBMYKg0W+DmE44kxU6Uxl2MdVHBuMkwgkmKUNYls9eSZsv3Z8y8Q2amHzpbtbnepbPPjFJpURSlqpRxRNbr+Pm4NWH7JzOfEtDeawRABttEMhgggOUMquTpo2RIqImmpZ//dOfp25upcggRbJW8Hje2UA1BRjRzcoSnUbl+5XLY3724S2fa29xztDqgncDwawmReHUoYgRTXldcoIThtdr0MGB7jEZ0Lvwv3sqF6i+ySqXK/28wV2IZ0bgmwV4YFTQlOvInsWSwDM6QB4/FKMJiijatbgz1IWTeB30Cdqoftg0fK9tnHzxHZ01/4UtleXVsnMqIikhSEYerhJ38J6ode03aTEZPhvtobzWCIANtoBkME2EYzGCIgw/yBNcZ2oJUTeYpqyDMIOn3Fva+kZMyFowV+zECY2uyYAdkru8ONsr1x4TXSb/gxMKlzxq4fgb21NyCiFthirY63T5odeq/wa++N6RiDLW8TtiCgs8Z0eq1iaT6F54SGE3NZYx7D6ZixdOCyFrrjWd5FLDdUKaUEvxfM68grlGJCIh4V4jLPwmiueTvs8JnHSLfPPP1s2T51+gyRNZsQVRCYS7RatRZl7LhG6shQwDFJr0Pv48MPnSrb9kYzGCLANprBEAEZVvrgAZFYBUUt4K6k7C6UxI6Sa56zP1B1nHDVEXJJJJsfl+1DdaY2rS/7dkoJpFtAwtgZ01yIgzs3y3YX8mVw1gWuazSiKkS/4SeoZ8CmYGMUJLklq5AC92CMLnGmsk2mfvztIXXbkwIsoHLWW4xBohSqJ3cfBhxxpgyoh62Vg0R27OEnyvajoB6eeOA06dfu+mMRbrpoqeYlL75WYSj4E9sH9Yb/nv3eEpGdfejBsm1vNIMhAmyjGQwRYBvNYIiADF3zlYBIpE+xCwvqG5U+sDFlG00KAuWfK65/sI3qqz75SmftAOnnwJbLE5pwpg/Jbjb2hkS2Aa76bhfsqzalCxWFH3/MbKPRBFz6tXRm2zkaPFrloUHCGRLsSnuNxmAPstMaTH9IKpSyueoYnDqmNjHpCjSuBgu+fODxZ8r2I09/gciOQ2BmGyIbuL2pVXBVq9EKyZAqP03xN0z/gN85Ze8m3D/9FfqbSwv/W7I3msEQAbbRDIYIyBLCLJBf3Wq2Q8Lkl92wlXTeeCqAqkzlKGF2/gbnnKvVvGqTHTzq/w5t55xrQsDlOOeljvyYwyFl9q/22mW7BSf/rS5VP4upVxd3K0XgQXUkDHo5/Th3X+MxDClQzp7McM/PlbLxh8AUmU6F4xnnXAH5PsYFDUBNgZFx6qnzZfvhZ54j/U5g8CXLmYi/uUSLHqFXsc8K5YhcpuiHislD1EVFhSXp3ltdKls+7MdQlmgwGPYJttEMhgiwjWYwRAAtrVtxKcuuUdprdkTr7DGlIeVjAMJM4lHaMH4+9bbFxWsfk34nnY9mXmGu+TrYLmusDOxew7ufHdhyPKLYJXBdk0ZYZ5CMBssvcRuK0Mkqqf3xHgMznvn3Sd7IMf0uWJ53BL7+CaNP5XD/l/rUZf3Isy+W7XNf/Krvt7JC+qEdWf0JSDY3j/CX7TftREmaS+um/WwTwdV/7zMek9Dn2ehAYiBtiQaDYX9gG81giIBM0dgINLY0fbXK6meuvuJlHy2W/eHubHSJ377uE/Bc+tMfSL+NzPc7dZCqOcsdYCew/ITjvenMdrFH2R+DoWf97+xSdkkNxlxd9q7uRp3ekKWOP0pAFdM5TrAB1jzLQ4kyPEpwzrksgfyVENzAVdhG10c6nPvqPxHZZ57xbPs23jftf7b63MM6aol1gofXLqx4/mebTZXft7Yv8EhmgeUZDIY5YRvNYIgA6nWcgxAs9lNerbzyIw0YJfU62QRYPZJloAAy78Y1rzq2UzpGt+UD9JKMMkMGwH64u0kDP9+4cqdsf3zHs0Zq7LscP+jVrTZLSTLe9mpmAZ6pIVMxjx7wYyz3O0TW7Xq1Eh2ve8xjiAGYPJAXe44xT2S9Tfo9dM7nTHzsmfNE1gJ1USeMF4KEQncYyswk+rutSn0LZDwHZuDcCHmmWTL/rO2NZjBEgG00gyECbKMZDBGQKYUwGbPacSE0ZYY06uoVdnaBtpec1JCOIdtoxcgHaa4uUxtnve/tkJWVZSLb2dku21c2aD78jW2fKz+Bm8XzS+7sejvvyKnDRLYH4+P3JEG3zrkx5m5MqKG3A/nqp2QdlWTwvs3GzyGIEyMiTpx9kvR78oWvl+1WhzLS6Y9EebakF7eNBLupmgBfGVOoxOr25+1RCGtc9CjB3mgGQwTYRjMYIiDThNLr895HfJ1qxGRFHSABo6jmULWMBETylzckNkkhyK/bocThbt+rQK0Wzes42PVz7+5Ql3sHjglWD/q8GHtDegyQO6/CTiZ0/UtLXlUdQcpxDFp1zrlrH98t2wf7tGpoA5giQ8x/wtStBArTZ/yYAUjRzZ5XrZ988W9Iv+4yMGd4IG8xW2XjAbnkGs7mkfppuRoVVK/CNeLxEusVxplXJ9PSjuD+sTeawRABttEMhgiwjWYwRECmaZnE81r1/c9GhYEFiVi4PSEQWCrldUCZrsRbQjsDFnqTBV+2mt4uy5jx0mz4vit96s7eHnj61NE1b2uNR5S9vwku/O1NmuCnd8jnm8wgJ/3ehDLvN4fePry5SUs/nVny+esbYGtN29SFj6va2t0lsmLkbbulFX8EsXb4GOknufDvfQpzwRfa70q+SOzH6WRa4Kc0s0afCl0jN0WTZLY9yCe0N5rBEAG20QyGCMiEeMK//GVGa4YM3fQV3Q5URy24jjD0eXAnMFR42misQNn0Ln0ezFjD/NhsHVillOfg2N7xytgepAtf6VLmyRjKPU0KWjJqOPBqYH/Zq58bO1Q9xKlvbmwQ2amj63750K+WUjU4zzHvCMsJDvfxwLETZbvRpt9FU7fIcFqwrhYMLB0FaAEi83TGqYJ6zXLNh80kp8ano9gbzWCIANtoBkMEZIkW3BkcE4rqIU8JDjJW2JwlxBbaLMUccz9hVZR6yxOHOemXsEvYv5fJFIqjszwbHaiGOQZWR71PU4KnqCZwFg2od0PwVo5YMXf8nluMoXLzzlbZbkNx91qTzjXBrHScbwxf/ODx075fIj+J++iOMJks46yRBQkgIhYdTstDIkPxXSr3yt5oBkME2EYzGCLANprBEAEZta+UxDpaTXvSlFn+FdtFSoDCEvDwUFI6hrehapl37/N8h2P43G5Q9v4Y2PC9LmX9P3bGsyZGA28ndVgGnt7Q22wbLOkOMlGu3vDlo4Z7tCQSBnHuTqkMx6zXfarp8YQyVGrAcimm1AZMG/67rR07Vbb1eAvFha+l7NaoG8L4c1lM+2znqawRtDfnmbcw977BEBW20QyGCAhmhqj5GoVLKpdVxsB9rpCKA/OWp3WvNo2Z2jQBAu90StXKAtgUHVZNZv2YD/a8ctmrtO0uJR8fTEFtvUkr2Qx2PWtkE3KS8COITgeODJjLPQdffaMDxGRWmL4Alst4TNXP3sHjZbt/wJOUq8ohBvwySbDvPwyEhaLob1XShZwzJDiAVOlGGR9ydVGtYileZm80gyECbKMZDBFgG81giABCweJUHNyHVbemZL8pxwAVieDer+RIBz24YGsEpnxvxdtTKWO1T6ay7VIHS5WZTSQotNfzFK92l1Kw+n0/3807d4nsKlQfzWGCXoseJTTAxux26Pg5cTFjtARl6KPdxyMRjp59ws/VpOMjVMtLsVdYR004cwwtuHMuaxANLFL7gXVLZL+AZDtqCXg02BvNYIgA22gGQwRkqGJpzJBqSvDZKidn6CcKs5+6b9PZgnuDwgeq24E3260c9u7rY489Q/oNr71XtrcGLCcjsEuajPGBrJE+5BNZXlki/e587AM1d3Zoro4GBKGugMrZ79EjgikcSayz8UdwJLFx1+cnwZyRn4xStvI6lR1/1Kf+5s+JQKnfLrFGNMtCG7+QjonuA72vIJ0rsHQ2qicQSjp8Y4YYDHFhG81giIAsrSlZwbVKnqSbEtwZSlrGfso68oQyPtBb1Gj5NNpnX6BprnfvejLvaJt6BSd7XtUbbNNUcR9der9sN3PPwthi6eBu3/KVQWs1qmAcOgQptkHUyCi5OclRdaRq350tT2ieQMBowVku4Ek7+tg5Ils+dNTNDf7IFsqjrY8ZhCr9Y/6p1IqfmndcWwiKeMCyqY4GQ1TYRjMYIsA2msEQAVkKVSELvu+UgE7STbPl0GarJO6ZndiEJ3NBdn1e8KQ7s33R3PbsHvB5EbtrB4hsOvFMkcE7F4nsz+9/WLa3PrpZtg8s07JKvbZndRw6uEZkQ7Cp2pCavJnRtOXNupdN2D3AO9fDkk4sSqHW98cCpz/3JSoT7HE9VbYs1BgToVQOnVmxmD0YXv5JDQ+Y2VwU9kYzGCLANprBEAEZ5tzgr9xCfbXOVhf5GIkiQ7WPtHM5J2NR8FyI8JlUBpXf91t3b5PPb/70lbL9/hu/JrLRwLv+c2Cl3LxDU3Z3ul4dPQCVQZ1z7vp1Type6fv02+0Gyw0JuUVu3KLj93peXUzge47ZvTr5yFNlu79Oi9YjQvOCaNDqvCsFaUR1cR4VNjRld+h44VdyF76TZZYS3GCIC9toBkME2EYzGCIgI4x6haKiV/xUaFY4RqDbnttoNBELkyGDXFHCP3zPs/d//t3/JLK929fLdqdBXe7LUOUT81Ly3JMYWPre5etEhmOurHg6Fi+5dHvTs/IzqCPgnHN1iAAYQn7JxoEjpN8D575YtjWGvkY/0lzu4RU/xeHF2NFqeSd53kTwEXAkip9BT/k02y6rfv8wS9LeaAZDBNhGMxgiIFPTfaivZ8Ftn3D3J7rwNbc9ymT1MOWJHzCfCLAk/nTxLdLr5W98o2wPb31EZMfWvXq4ukQDLjttz9bAIwOeN/IOlFn68Do9Pjh9/FDZvn7b53WcsGLx+K0xf4hztKJoCrlGHv3K35N+3VWoDKqp8QrzIVF98/MHVWq+/8rjnN2NBQaHT60L4bepqLdO/nmLTBk6ur3RDIYosI1mMERAlqay7lho711JvSi4xzAPkiVchlMRxyj93zDY8arYL3/0f2X716/8iPQbQeBkt069faiWNFkatkbdf8/B0Ad77k6onlBb8erh6pSmgNuGHCLDXe+RTFjh+24bPI0dGhTaaPk1P/jc18v2kYefIP2oGu8oCAlY7lcoFdATSafSGBNqwKXC8Qimcsjj6/xiJV+JoC5yFVMrFm8pwQ2GyLCNZjBEgG00gyECqHtfqblUDfxEtr12co65/5gMba9CmQs+3rx2jYhe+e63yvblt98s2zTZtnOrq579vtTtEFmn5e2hfEJzPg6AvTKu+X4PnH+B9NuD0kx/+ugVIsvAVY9u+0ZGbcVm3dt2GJDrnHMPvvB3vv25F8t2jQWPcvYNgeK1J90IAyaUCaHYcmps5+zg3/tBZ3XsB+SyUGQdinsfhfZGMxgiwDaawRABmUbOpMXAKYhKkSiE4ArLY/b4SIDNp3S2P174fdl+5TuUELxz80bZXgUWx0qXknL7Pe+277Z4JRWvHg52tomkd/hU2X7yxX8o2+MJ/V4/+/d/KdutOlX7mlCcvgWsjmZG+6E/+PizLxLRg5//StmuZXIuTs0UUNVKcbwwknjVsAjUU8lk/KOWp0a8TBsyWBh4inEfUrSRig2GqLCNZjBEgG00gyECMkIDUhj6nHlf9WX+ZYhAOrajOvjmXZ8P//VXXyX9fvGD7/sFT0dEtr7sXfXrS74M0mqfuvBbTXCDJ/S77BXe5jnJbKPT578Ky/f9fvhv/0z65bueCtbt0Ll7cJzQbnobrSho5dHWiYfK9kPPf43IuBtfxvzObsW02J+ZtIBiYufNsXbl9ECPSJmNBdle9LvwLQJte6MZDBFgG81giIAsJdR4uu9QhUi5DFROzOOhqR08nff7F3367Ve/7d32Ny9fIv06cF2fue2PrPocHGtLqKLR9U4h/2Nj/RiRnXryeT/GybNEljW8qvfbH/6gbF99523Sb21JZp70QJWc5l5dnDRpWvGzX/bHB81O3+03JBbDPmS81sdQyCXB6mKl2/wqp85x4cGpKMKjLDmvCY8KTY29bzDEhW00gyECsrSGe43vO1QPQ5NIU8kEKrVceO01Ivv5t7y6mA681+5EnxZRX1ry6mKdsSnaECBZhyDNokXVtyNPPFu2189+lsjqHa/C1VLq3bt7y1eQufiaDyZtssosHWB/1BLO+PDNARSfP/Pi10m31WMPuBCoRHAFwWm0Nc+xsJDqJYG/EI2doU0dmkYuMD9JdfzZ6qLukZUXYm80gyECbKMZDBFgG81giIAMWfNVpraXhaq3WD3TOed+99OflO2f/8c3iawPSvPqui91tMRc+F1g3hc1mk9xBNEC/VMPl+2Tn6cMj+7awbKdpNSGws/TKT2CuPDj/y3b+c6mX+/yMunXb3ubsMGCNnd3fVKf3gP++ODEZz9P10GOULhNvAAbngHtmiJB9zgFmaviEw89F5CTIcqho2Hpte/Xl9hlNCPRHJh9FqKxaJRcSPZGMxhiwDaawRABGVZI4TkTiXtfee2OR57o+wawJ5xz7sL3v1e2j7ap63wFGBSY0xBzeDjnXFLz6ty0TbOBnDjnC6Iffuxc2a6xqjCYu7FSZQXIwpcuvk5EVy78qmwvdfzcKz3K6ljqePW2xlWlhmevPPJln/ujwY4g6EWMiIsam0JUCHaXKw+UjrkfDOO/PquH/jVDx9e+yz6o48qY9kYzGCLANprBEAG20QyGCMi0LCca0wcD9t799S/L9jsvf5/0OwIJczDo0TnnGhDM2GxCeaSMTpZCKaUTz71EZGtnPgNr1OwOtNGo+33r9q2yfeHl/yKyDgy51PF2ZJfZmw1Yc85s3Qee91SrAydPw6LE5Vbc19Qdr3xPeUjaT+lYKD5rzL+JZh6nNxXBxwDiVPS4aS4TKrSzUmMAey3wXXhne6MZDBFgG81giIAsUV6fmks/n/pqlXff9UGQ603Kau+iSshUthqUT0obvt1/iAZfHn3Wp9/urB9yEjSVB4NaJ2PKXnn9B98p24MbV4lsrecjCZYhD0mbHUHg+Ieeep6Ijj31Bd9NyZWpq4SBiQyDIetAWsVP8dei5dKoqJ+zPxSs4zxse3KddH8qsZ37cSOVRVrgp8EQF7bRDIYIyArCDAl/labArjh85HDZbty4TPo1oQJLmlF1awqq4+pTntVx7DwlBGct7+2rBt5plWw8ciAfv/nqK0T27s888XmJpfNugerbgXTeKeu38rhXD4+ff4nIanWmZn6C/VBc5nhk4ux6qnCmzgkarFZ1prJEId+2Ttjl69iH9OaBefVUFVPxSOLesjeawRABttEMhgiwjWYwRAAp2zSXug+u+hPnv1i2OykNnCw2farvrLdCZN2Hn/BtCNpM6nL664pNUpCyoeJ1Vy76aqDvvPw9IlvJ/Bgtlnq7Dkl4kPXfO/046XccyyrVeb1RARoTR4FuWYQ9Ty3wU71STAWvhT3yIWaPr5zIzBhE/p77kadSPOCoTCYfu2hprwwGw6cA22gGQwRkqrqikXSh3VxZL9tHX/pH0q+Y+KDQhKlUqGIVCmMidImoBm/duU36/e5/vl2222NakWap70nLyytLRFYDxkrS9arvsWdfIv2yJuQ50bzIwnrnAc1bKEurKpVATJ7H7S39JpRKmJV8HyLzRFYCK/eKni0wkfRbCv9lBfdUCDuWM8RgiAzbaAZDBNhGMxgigFLtF6TzkKQ+dTpkCmWPqnr2/A5tza7BqqTvvPZjIhte+bBsH+jQvJGrYKN1WcmlCdhox770t2W7vbwmrHBWAKO4ZBkK4z04MU3FrBFsI6VkpkZr0740NaFkYaJEj9KPetElaWH6m0T+noEBAOqa8JO90QyGCLCNZjBEQKa7m8OCDRMl7wgdI0yHSlQ1QX49b3x0o2xfe+MXpN9az6uESx2ak7EFrHyacNy5tSc9K38F2CtVsjqqYtr3lIMq1RwZi6ifwSm7+XWBqro6AbLyFWa/fDsqs8lThd0cPUoheLbgq0x1NBgiwzaawRABttEMhgggFCyt4Hw10la2m+SrZFBVXXEHazbahx+U7dZoQPott32SnV67RWQZlMVtnnmYyA499ZyfCyIW9Khb9UbKUJLqS655zSSujj97SdWghzD3vprEJ7B+bkFKIlVClMXrQvPoS5EC9x9jflSPryyvo8EQFbbRDIYIyHSGPqgQgfWkNyXUAAAFdklEQVSB5mF/UG82ss652hTm2u0ve3b9wVVakbMHDJWsQaMI0oM+udDR575GZFkT1cyw447qGsMEmpeaiAKfmT6IxgzRxkO1TBycue3Dnl81plKpShp6dBF4BFHtJyQQCp3WUZXc3mgGQwTYRjMYIoB6HedJYiG98iveMlkWPBVRYeV+aw/6VOLF818msumNP5ft2spBIlt+2uc8aS6tEhkurNCYG04TSQGXfKr5VdO5npl4Hc+Vjd+Th05KrI55mBth1Vk08rTOKJm/fE11+fNTcTQPsL3RDIYIsI1mMESAbTSDIQKSS+/8Fo/miZAwNLQke5o7OBE/BNtv8kXKsQCUlXLOuXxvt2ynTcoMSbLZufGrc2tM9r8eUqKhT/4w88M8lsRCa6zYaGH9ENU1CvUS5lmgFnRaSEcQWq7Jvz4Pv/Y97Y1mMESAbTSDIQJogg9VXVHcvIGBmRUorADlIvYJCbDw9xr9arVOP3BR8nx4WT6HO1uabp4yWSHjcejKUFg+RX1u7ThCKWcUNHZ43GoowVgdjptN4vNdLHjU3mgGQwTYRjMYIsA2msEQAVlo0GZwrhiNh7IYW+g+rn+J4c3+hwjlXKufw9jeaeWYIQyaXabeDzIBlMZSoip0m0cnvQUOIkNxq0szV4NMw34hC1TZDRhU5Wf5boFBsvZGMxgiwDaawRABme7mlSVSoUMepBns7A9lkFSuW8xFHjqetCye14SmdZTLCIlj30cY7lYPxKLqlqBl6zkeA4dWc618Gvohjr5oJsew6+yNZjBEgG00gyECsiTV1BD0qMgqVSJ+mDnoTOi9FvFczsEWUNRWSTXVnHGJEp2q5gUJlBWS/savqTyz2enstEDVqkgsaxOOBXnECDUYmKSwk72H1GjihPr5V6bdDnujGQwRYBvNYIgA22gGQwRkGvNedyPPvk610BZ1xZP8fvvgzldc+GoGaQTT9xdh4gdyM+YcQ7Gh9sNdLpqHlehLeapkdjd9LuWOhJZj4s8dA0SVqdVFBqYctzeawRABttEMhgjI1BfffpAu9kE/WmQZi+c75ED1AnWeBUNVg7m8yvqLwH6KjFZxYevQVMx9cc3jB3DTz1GRM1xFDhoh/KK5YKqjwRAVttEMhgiwjWYwREBGGCqKuq+x98Xc8kzGITPSOYs7zAccbAsFjfbJ1EQmRzqo+r6QE1Mz8yoUICFcQg+VVO69Moaa/5DYinJZpWBojK59KM0UmjdSt1NTSUDuh2Zj2hvNYIgA22gGQwRkOoNZhqhGqd5mWdGpFAon3VQlVpHNlmiRCItl7VMmm3PU8gqufQpMC5WRomjg2h3VmP2JUNC+ojYVQnvmhIJAYZfo7HpBpVV+Y3z9NBdIzrvPDXujGQwRYBvNYIiAjKgeFbVPCw4UL1Nk+0E1CUNVo5KJz+p1whevaEPBKfFC17EPzBbmRpYqdM73bIWFaOqyck8LmmxFWQmbjnj7wu59ZQyNKyymMZynnzFDDIaosI1mMESAbTSDIQLUwM8wx/l9EFyaSXU4l6iq1ULfebJLK6x51MGLBe9IqF0WakdK19wXAnNhrmcrRQColUEXO5JRI0TJR9n9njjZFi2U4wNyfKWe5ITdPXujGQwRYBvNYIiATBMmGkNAvmiO6WerStW5ArkbWuAkjqC4onVX94IKtODB5qPpStR+HI1I91FWlytKn8j4UNg2BVXtaFyppkYuQCavTi6PEEhaDjcZuPps7n2DISpsoxkMEWAbzWCIAGajKex6dZgw1rw2CNGCFRsq2D0+l9c7VO9WglOFvPbsKi3QgSJ0/Yuxj9QLKY1LFqrVLgODG2ggqXxNbMhBnHOcG0FXe6MZDBFgG81giID/B3tXMFExRbEeAAAAAElFTkSuQmCC\" y=\"-8.740469\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"md4081da3c1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"28.62375\" xlink:href=\"#md4081da3c1\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(25.4425 241.338906)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.59875\" xlink:href=\"#md4081da3c1\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(56.23625 241.338906)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.57375\" xlink:href=\"#md4081da3c1\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(90.21125 241.338906)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.54875\" xlink:href=\"#md4081da3c1\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(124.18625 241.338906)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.52375\" xlink:href=\"#md4081da3c1\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(158.16125 241.338906)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"198.49875\" xlink:href=\"#md4081da3c1\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(192.13625 241.338906)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.47375\" xlink:href=\"#md4081da3c1\" y=\"226.740469\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(226.11125 241.338906)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"md4684f3212\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md4684f3212\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md4684f3212\" y=\"44.974219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 48.773437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md4684f3212\" y=\"78.949219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 82.748437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md4684f3212\" y=\"112.924219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 30 -->\n      <g transform=\"translate(7.2 116.723437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md4684f3212\" y=\"146.899219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 40 -->\n      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md4684f3212\" y=\"180.874219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 50 -->\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md4684f3212\" y=\"214.849219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 60 -->\n      <g transform=\"translate(7.2 218.648437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 226.740469 \nL 26.925 9.300469 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 226.740469 \nL 244.365 9.300469 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 226.740469 \nL 244.365 226.740469 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 9.300469 \nL 244.365 9.300469 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pc1b2e4927c\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"9.300469\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 6\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X, y):\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y_onehot = tf.one_hot(y, depth=6)\n",
    "    return X, y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = tf.data.Dataset.from_tensor_slices((X_train_orig, Y_train_orig))\n",
    "test_X = tf.data.Dataset.from_tensor_slices((X_test_orig, Y_test_orig))\n",
    "train_db = train_X.shuffle(2000).batch(64).map(preprocessing)\n",
    "test_db = test_X.shuffle(2000).batch(64).map(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(TensorShape([64, 64, 64, 3]), TensorShape([64, 6]))"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "sample = next(iter(train_db))\n",
    "sample[0].shape, sample[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'glorot_uniform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbias_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \n2D convolution layer (e.g. spatial convolution over images).\n\nThis layer creates a convolution kernel that is convolved\nwith the layer input to produce a tensor of\noutputs. If `use_bias` is True,\na bias vector is created and added to the outputs. Finally, if\n`activation` is not `None`, it is applied to the outputs as well.\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument `input_shape`\n(tuple of integers, does not include the sample axis),\ne.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\nin `data_format=\"channels_last\"`.\n\nArguments:\n  filters: Integer, the dimensionality of the output space\n    (i.e. the number of output filters in the convolution).\n  kernel_size: An integer or tuple/list of 2 integers, specifying the\n    height and width of the 2D convolution window.\n    Can be a single integer to specify the same value for\n    all spatial dimensions.\n  strides: An integer or tuple/list of 2 integers,\n    specifying the strides of the convolution along the height and width.\n    Can be a single integer to specify the same value for\n    all spatial dimensions.\n    Specifying any stride value != 1 is incompatible with specifying\n    any `dilation_rate` value != 1.\n  padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n  data_format: A string,\n    one of `channels_last` (default) or `channels_first`.\n    The ordering of the dimensions in the inputs.\n    `channels_last` corresponds to inputs with shape\n    `(batch, height, width, channels)` while `channels_first`\n    corresponds to inputs with shape\n    `(batch, channels, height, width)`.\n    It defaults to the `image_data_format` value found in your\n    Keras config file at `~/.keras/keras.json`.\n    If you never set it, then it will be \"channels_last\".\n  dilation_rate: an integer or tuple/list of 2 integers, specifying\n    the dilation rate to use for dilated convolution.\n    Can be a single integer to specify the same value for\n    all spatial dimensions.\n    Currently, specifying any `dilation_rate` value != 1 is\n    incompatible with specifying any stride value != 1.\n  activation: Activation function to use.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: `a(x) = x`).\n  use_bias: Boolean, whether the layer uses a bias vector.\n  kernel_initializer: Initializer for the `kernel` weights matrix.\n  bias_initializer: Initializer for the bias vector.\n  kernel_regularizer: Regularizer function applied to\n    the `kernel` weights matrix.\n  bias_regularizer: Regularizer function applied to the bias vector.\n  activity_regularizer: Regularizer function applied to\n    the output of the layer (its \"activation\")..\n  kernel_constraint: Constraint function applied to the kernel matrix.\n  bias_constraint: Constraint function applied to the bias vector.\n\nInput shape:\n  4D tensor with shape:\n  `(samples, channels, rows, cols)` if data_format='channels_first'\n  or 4D tensor with shape:\n  `(samples, rows, cols, channels)` if data_format='channels_last'.\n\nOutput shape:\n  4D tensor with shape:\n  `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n  or 4D tensor with shape:\n  `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n  `rows` and `cols` values might have changed due to padding.\n\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     Conv2DTranspose, DepthwiseConv2D, Conv2D\n",
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "layers.Conv2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nMax pooling operation for spatial data.\n\nArguments:\n  pool_size: integer or tuple of 2 integers,\n    factors by which to downscale (vertical, horizontal).\n    `(2, 2)` will halve the input in both spatial dimension.\n    If only one integer is specified, the same window length\n    will be used for both dimensions.\n  strides: Integer, tuple of 2 integers, or None.\n    Strides values.\n    If None, it will default to `pool_size`.\n  padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n  data_format: A string,\n    one of `channels_last` (default) or `channels_first`.\n    The ordering of the dimensions in the inputs.\n    `channels_last` corresponds to inputs with shape\n    `(batch, height, width, channels)` while `channels_first`\n    corresponds to inputs with shape\n    `(batch, channels, height, width)`.\n    It defaults to the `image_data_format` value found in your\n    Keras config file at `~/.keras/keras.json`.\n    If you never set it, then it will be \"channels_last\".\n\nInput shape:\n  - If `data_format='channels_last'`:\n    4D tensor with shape `(batch_size, rows, cols, channels)`.\n  - If `data_format='channels_first'`:\n    4D tensor with shape `(batch_size, channels, rows, cols)`.\n\nOutput shape:\n  - If `data_format='channels_last'`:\n    4D tensor with shape `(batch_size, pooled_rows, pooled_cols, channels)`.\n  - If `data_format='channels_first'`:\n    4D tensor with shape `(batch_size, channels, pooled_rows, pooled_cols)`.\n\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/pooling.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     MaxPooling2D\n",
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "layers.MaxPool2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Conv2D(8, kernel_size=(4, 4), strides=1, padding='SAME', activation='relu'),\n",
    "    layers.MaxPool2D(pool_size=(8, 8), strides=8, padding='SAME'),\n",
    "    layers.Conv2D(16, kernel_size=(2, 2), strides=1, padding='SAME', activation='relu'),\n",
    "    layers.MaxPool2D(pool_size=(4, 4), strides=4, padding='SAME'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(6)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              multiple                  392       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) multiple                  0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            multiple                  528       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 multiple                  0         \n_________________________________________________________________\nflatten (Flatten)            multiple                  0         \n_________________________________________________________________\ndense (Dense)                multiple                  390       \n=================================================================\nTotal params: 1,310\nTrainable params: 1,310\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.build(input_shape=(None, 64, 64, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m\n\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mweighted_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtarget_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdistribute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nConfigures the model for training.\n\nArguments:\n    optimizer: String (name of optimizer) or optimizer instance.\n        See `tf.keras.optimizers`.\n    loss: String (name of objective function), objective function or\n        `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective\n        function is any callable with the signature\n        `scalar_loss = fn(y_true, y_pred)`. If the model has multiple\n        outputs, you can use a different loss on each output by passing a\n        dictionary or a list of losses. The loss value that will be\n        minimized by the model will then be the sum of all individual\n        losses.\n    metrics: List of metrics to be evaluated by the model during training\n        and testing. Typically you will use `metrics=['accuracy']`.\n        To specify different metrics for different outputs of a\n        multi-output model, you could also pass a dictionary, such as\n        `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.\n        You can also pass a list (len = len(outputs)) of lists of metrics\n        such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or\n        `metrics=['accuracy', ['accuracy', 'mse']]`.\n    loss_weights: Optional list or dictionary specifying scalar\n        coefficients (Python floats) to weight the loss contributions\n        of different model outputs.\n        The loss value that will be minimized by the model\n        will then be the *weighted sum* of all individual losses,\n        weighted by the `loss_weights` coefficients.\n        If a list, it is expected to have a 1:1 mapping\n        to the model's outputs. If a tensor, it is expected to map\n        output names (strings) to scalar coefficients.\n    sample_weight_mode: If you need to do timestep-wise\n        sample weighting (2D weights), set this to `\"temporal\"`.\n        `None` defaults to sample-wise weights (1D).\n        If the model has multiple outputs, you can use a different\n        `sample_weight_mode` on each output by passing a\n        dictionary or a list of modes.\n    weighted_metrics: List of metrics to be evaluated and weighted\n        by sample_weight or class_weight during training and testing.\n    target_tensors: By default, Keras will create placeholders for the\n        model's target, which will be fed with the target data during\n        training. If instead you would like to use your own\n        target tensors (in turn, Keras will not expect external\n        Numpy data for these targets at training time), you\n        can specify them via the `target_tensors` argument. It can be\n        a single tensor (for a single-output model), a list of tensors,\n        or a dict mapping output names to target tensors.\n    distribute: NOT SUPPORTED IN TF 2.0, please create and compile the\n        model under distribution strategy scope instead of passing it to\n        compile.\n    **kwargs: Any additional arguments.\n\nRaises:\n    ValueError: In case of invalid arguments for\n        `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\n\u001b[0;31mType:\u001b[0m      method\n",
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model.compile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m\n\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nTrains the model for a fixed number of epochs (iterations on a dataset).\n\nArguments:\n    x: Input data. It could be:\n      - A Numpy array (or array-like), or a list of arrays\n        (in case the model has multiple inputs).\n      - A TensorFlow tensor, or a list of tensors\n        (in case the model has multiple inputs).\n      - A dict mapping input names to the corresponding array/tensors,\n        if the model has named inputs.\n      - A `tf.data` dataset. Should return a tuple\n        of either `(inputs, targets)` or\n        `(inputs, targets, sample_weights)`.\n      - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n        or `(inputs, targets, sample weights)`.\n      A more detailed description of unpacking behavior for iterator types\n      (Dataset, generator, Sequence) is given below.\n    y: Target data. Like the input data `x`,\n      it could be either Numpy array(s) or TensorFlow tensor(s).\n      It should be consistent with `x` (you cannot have Numpy inputs and\n      tensor targets, or inversely). If `x` is a dataset, generator,\n      or `keras.utils.Sequence` instance, `y` should\n      not be specified (since targets will be obtained from `x`).\n    batch_size: Integer or `None`.\n        Number of samples per gradient update.\n        If unspecified, `batch_size` will default to 32.\n        Do not specify the `batch_size` if your data is in the\n        form of symbolic tensors, datasets,\n        generators, or `keras.utils.Sequence` instances (since they generate\n        batches).\n    epochs: Integer. Number of epochs to train the model.\n        An epoch is an iteration over the entire `x` and `y`\n        data provided.\n        Note that in conjunction with `initial_epoch`,\n        `epochs` is to be understood as \"final epoch\".\n        The model is not trained for a number of iterations\n        given by `epochs`, but merely until the epoch\n        of index `epochs` is reached.\n    verbose: 0, 1, or 2. Verbosity mode.\n        0 = silent, 1 = progress bar, 2 = one line per epoch.\n        Note that the progress bar is not particularly useful when\n        logged to a file, so verbose=2 is recommended when not running\n        interactively (eg, in a production environment).\n    callbacks: List of `keras.callbacks.Callback` instances.\n        List of callbacks to apply during training.\n        See `tf.keras.callbacks`.\n    validation_split: Float between 0 and 1.\n        Fraction of the training data to be used as validation data.\n        The model will set apart this fraction of the training data,\n        will not train on it, and will evaluate\n        the loss and any model metrics\n        on this data at the end of each epoch.\n        The validation data is selected from the last samples\n        in the `x` and `y` data provided, before shuffling. This argument is\n        not supported when `x` is a dataset, generator or\n       `keras.utils.Sequence` instance.\n    validation_data: Data on which to evaluate\n        the loss and any model metrics at the end of each epoch.\n        The model will not be trained on this data.\n        `validation_data` will override `validation_split`.\n        `validation_data` could be:\n          - tuple `(x_val, y_val)` of Numpy arrays or tensors\n          - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n          - dataset\n        For the first two cases, `batch_size` must be provided.\n        For the last case, `validation_steps` could be provided.\n    shuffle: Boolean (whether to shuffle the training data\n        before each epoch) or str (for 'batch').\n        'batch' is a special option for dealing with the\n        limitations of HDF5 data; it shuffles in batch-sized chunks.\n        Has no effect when `steps_per_epoch` is not `None`.\n    class_weight: Optional dictionary mapping class indices (integers)\n        to a weight (float) value, used for weighting the loss function\n        (during training only).\n        This can be useful to tell the model to\n        \"pay more attention\" to samples from\n        an under-represented class.\n    sample_weight: Optional Numpy array of weights for\n        the training samples, used for weighting the loss function\n        (during training only). You can either pass a flat (1D)\n        Numpy array with the same length as the input samples\n        (1:1 mapping between weights and samples),\n        or in the case of temporal data,\n        you can pass a 2D array with shape\n        `(samples, sequence_length)`,\n        to apply a different weight to every timestep of every sample.\n        In this case you should make sure to specify\n        `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not\n        supported when `x` is a dataset, generator, or\n       `keras.utils.Sequence` instance, instead provide the sample_weights\n        as the third element of `x`.\n    initial_epoch: Integer.\n        Epoch at which to start training\n        (useful for resuming a previous training run).\n    steps_per_epoch: Integer or `None`.\n        Total number of steps (batches of samples)\n        before declaring one epoch finished and starting the\n        next epoch. When training with input tensors such as\n        TensorFlow data tensors, the default `None` is equal to\n        the number of samples in your dataset divided by\n        the batch size, or 1 if that cannot be determined. If x is a\n        `tf.data` dataset, and 'steps_per_epoch'\n        is None, the epoch will run until the input dataset is exhausted.\n        This argument is not supported with array inputs.\n    validation_steps: Only relevant if `validation_data` is provided and\n        is a `tf.data` dataset. Total number of steps (batches of\n        samples) to draw before stopping when performing validation\n        at the end of every epoch. If 'validation_steps' is None, validation\n        will run until the `validation_data` dataset is exhausted. In the\n        case of a infinite dataset, it will run into a infinite loop.\n        If 'validation_steps' is specified and only part of the dataset\n        will be consumed, the evaluation will start from the beginning of\n        the dataset at each epoch. This ensures that the same validation\n        samples are used every time.\n    validation_freq: Only relevant if validation data is provided. Integer\n        or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n        If an integer, specifies how many training epochs to run before a\n        new validation run is performed, e.g. `validation_freq=2` runs\n        validation every 2 epochs. If a Container, specifies the epochs on\n        which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n        validation at the end of the 1st, 2nd, and 10th epochs.\n    max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n        input only. Maximum size for the generator queue.\n        If unspecified, `max_queue_size` will default to 10.\n    workers: Integer. Used for generator or `keras.utils.Sequence` input\n        only. Maximum number of processes to spin up\n        when using process-based threading. If unspecified, `workers`\n        will default to 1. If 0, will execute the generator on the main\n        thread.\n    use_multiprocessing: Boolean. Used for generator or\n        `keras.utils.Sequence` input only. If `True`, use process-based\n        threading. If unspecified, `use_multiprocessing` will default to\n        `False`. Note that because this implementation relies on\n        multiprocessing, you should not pass non-picklable arguments to\n        the generator as they can't be passed easily to children processes.\n    **kwargs: Used for backwards compatibility.\n\nUnpacking behavior for iterator-like inputs:\n    A common pattern is to pass a tf.data.Dataset, generator, or\n  tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n  yield not only features (x) but optionally targets (y) and sample weights.\n  Keras requires that the output of such iterator-likes be unambiguous. The\n  iterator should return a tuple of length 1, 2, or 3, where the optional\n  second and third elements will be used for y and sample_weight\n  respectively. Any other type provided will be wrapped in a length one\n  tuple, effectively treating everything as 'x'. When yielding dicts, they\n  should still adhere to the top-level tuple structure.\n  e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n  features, targets, and weights from the keys of a single dict.\n    A notable unsupported data type is the namedtuple. The reason is that\n  it behaves like both an ordered datatype (tuple) and a mapping\n  datatype (dict). So given a namedtuple of the form:\n      `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n  it is ambiguous whether to reverse the order of the elements when\n  interpreting the value. Even worse is a tuple of the form:\n      `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n  where it is unclear if the tuple was intended to be unpacked into x, y,\n  and sample_weight or passed through as a single element to `x`. As a\n  result the data processing code will simply raise a ValueError if it\n  encounters a namedtuple. (Along with instructions to remedy the issue.)\n\nReturns:\n    A `History` object. Its `History.history` attribute is\n    a record of training loss values and metrics values\n    at successive epochs, as well as validation loss values\n    and validation metrics values (if applicable).\n\nRaises:\n    RuntimeError: If the model was never compiled.\n    ValueError: In case of mismatch between the provided input data\n        and what the model expects.\n\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\n\u001b[0;31mType:\u001b[0m      method\n",
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "model.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 17 steps, validate for 2 steps\nEpoch 1/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.7993 - accuracy: 0.7315\nEpoch 2/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.7830 - accuracy: 0.7435 - val_loss: 0.8409 - val_accuracy: 0.7000\nEpoch 3/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.7758 - accuracy: 0.7380\nEpoch 4/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.7689 - accuracy: 0.7583 - val_loss: 0.8486 - val_accuracy: 0.6833\nEpoch 5/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.7670 - accuracy: 0.7528\nEpoch 6/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.7432 - accuracy: 0.7528 - val_loss: 0.8150 - val_accuracy: 0.6833\nEpoch 7/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.7398 - accuracy: 0.7519\nEpoch 8/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.7396 - accuracy: 0.7528 - val_loss: 0.7953 - val_accuracy: 0.6917\nEpoch 9/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.7208 - accuracy: 0.7676\nEpoch 10/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.7008 - accuracy: 0.7759 - val_loss: 0.7840 - val_accuracy: 0.7250\nEpoch 11/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.6954 - accuracy: 0.7833\nEpoch 12/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.6965 - accuracy: 0.7769 - val_loss: 0.7638 - val_accuracy: 0.7250\nEpoch 13/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.6834 - accuracy: 0.7806\nEpoch 14/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.7028 - accuracy: 0.7750 - val_loss: 0.7614 - val_accuracy: 0.7000\nEpoch 15/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.6760 - accuracy: 0.7806\nEpoch 16/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.6616 - accuracy: 0.7898 - val_loss: 0.7387 - val_accuracy: 0.7167\nEpoch 17/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.6486 - accuracy: 0.7972\nEpoch 18/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.6463 - accuracy: 0.8009 - val_loss: 0.7213 - val_accuracy: 0.7083\nEpoch 19/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.6341 - accuracy: 0.8083\nEpoch 20/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.6286 - accuracy: 0.8111 - val_loss: 0.7067 - val_accuracy: 0.7250\nEpoch 21/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.6227 - accuracy: 0.8157\nEpoch 22/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.6140 - accuracy: 0.8120 - val_loss: 0.6901 - val_accuracy: 0.7250\nEpoch 23/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.6108 - accuracy: 0.8102\nEpoch 24/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.6150 - accuracy: 0.8120 - val_loss: 0.7063 - val_accuracy: 0.7167\nEpoch 25/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.6005 - accuracy: 0.8148\nEpoch 26/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.5874 - accuracy: 0.8250 - val_loss: 0.6959 - val_accuracy: 0.7250\nEpoch 27/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.5870 - accuracy: 0.8194\nEpoch 28/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.5940 - accuracy: 0.8037 - val_loss: 0.6890 - val_accuracy: 0.7417\nEpoch 29/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.5815 - accuracy: 0.8250\nEpoch 30/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.5800 - accuracy: 0.8185 - val_loss: 0.6927 - val_accuracy: 0.7083\nEpoch 31/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.5644 - accuracy: 0.8241\nEpoch 32/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.5648 - accuracy: 0.8370 - val_loss: 0.6607 - val_accuracy: 0.7000\nEpoch 33/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.5600 - accuracy: 0.8278\nEpoch 34/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.5552 - accuracy: 0.8296 - val_loss: 0.6645 - val_accuracy: 0.7250\nEpoch 35/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.5458 - accuracy: 0.8343\nEpoch 36/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.5396 - accuracy: 0.8380 - val_loss: 0.6319 - val_accuracy: 0.7250\nEpoch 37/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.5349 - accuracy: 0.8417\nEpoch 38/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.5268 - accuracy: 0.8389 - val_loss: 0.6340 - val_accuracy: 0.7417\nEpoch 39/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.5193 - accuracy: 0.8565\nEpoch 40/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.5158 - accuracy: 0.8500 - val_loss: 0.6404 - val_accuracy: 0.7333\nEpoch 41/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.5266 - accuracy: 0.8222\nEpoch 42/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.5151 - accuracy: 0.8398 - val_loss: 0.6272 - val_accuracy: 0.7667\nEpoch 43/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.5061 - accuracy: 0.8407\nEpoch 44/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.5069 - accuracy: 0.8472 - val_loss: 0.6200 - val_accuracy: 0.7667\nEpoch 45/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.4990 - accuracy: 0.8435\nEpoch 46/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.4944 - accuracy: 0.8454 - val_loss: 0.6169 - val_accuracy: 0.7667\nEpoch 47/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.4970 - accuracy: 0.8435\nEpoch 48/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.4948 - accuracy: 0.8417 - val_loss: 0.5874 - val_accuracy: 0.7417\nEpoch 49/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.4791 - accuracy: 0.8537\nEpoch 50/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.4842 - accuracy: 0.8481 - val_loss: 0.5948 - val_accuracy: 0.7333\nEpoch 51/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.4696 - accuracy: 0.8546\nEpoch 52/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.4671 - accuracy: 0.8556 - val_loss: 0.5786 - val_accuracy: 0.7500\nEpoch 53/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.4708 - accuracy: 0.8463\nEpoch 54/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.4679 - accuracy: 0.8611 - val_loss: 0.5913 - val_accuracy: 0.7250\nEpoch 55/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.4671 - accuracy: 0.8593\nEpoch 56/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.4628 - accuracy: 0.8528 - val_loss: 0.5686 - val_accuracy: 0.7333\nEpoch 57/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.4518 - accuracy: 0.8565\nEpoch 58/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.4511 - accuracy: 0.8630 - val_loss: 0.5821 - val_accuracy: 0.7500\nEpoch 59/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.4589 - accuracy: 0.8463\nEpoch 60/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.4437 - accuracy: 0.8676 - val_loss: 0.5701 - val_accuracy: 0.7667\nEpoch 61/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.4398 - accuracy: 0.8593\nEpoch 62/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.4342 - accuracy: 0.8704 - val_loss: 0.5509 - val_accuracy: 0.7583\nEpoch 63/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.4356 - accuracy: 0.8685\nEpoch 64/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.4430 - accuracy: 0.8630 - val_loss: 0.5764 - val_accuracy: 0.7833\nEpoch 65/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.4311 - accuracy: 0.8630\nEpoch 66/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.4271 - accuracy: 0.8731 - val_loss: 0.5836 - val_accuracy: 0.7667\nEpoch 67/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.4320 - accuracy: 0.8630\nEpoch 68/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.4198 - accuracy: 0.8685 - val_loss: 0.5506 - val_accuracy: 0.7750\nEpoch 69/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.4134 - accuracy: 0.8713\nEpoch 70/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.4141 - accuracy: 0.8741 - val_loss: 0.5443 - val_accuracy: 0.7583\nEpoch 71/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.4084 - accuracy: 0.8796\nEpoch 72/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.4008 - accuracy: 0.8815 - val_loss: 0.5435 - val_accuracy: 0.7750\nEpoch 73/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.4041 - accuracy: 0.8750\nEpoch 74/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.3967 - accuracy: 0.8796 - val_loss: 0.5307 - val_accuracy: 0.7583\nEpoch 75/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.4018 - accuracy: 0.8741\nEpoch 76/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.3935 - accuracy: 0.8750 - val_loss: 0.5407 - val_accuracy: 0.8000\nEpoch 77/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.3968 - accuracy: 0.8778\nEpoch 78/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.3940 - accuracy: 0.8787 - val_loss: 0.5346 - val_accuracy: 0.7750\nEpoch 79/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.3922 - accuracy: 0.8750\nEpoch 80/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.3864 - accuracy: 0.8787 - val_loss: 0.5411 - val_accuracy: 0.7750\nEpoch 81/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.3840 - accuracy: 0.8917\nEpoch 82/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.3857 - accuracy: 0.8750 - val_loss: 0.5025 - val_accuracy: 0.7917\nEpoch 83/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.3735 - accuracy: 0.8870\nEpoch 84/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.3820 - accuracy: 0.8880 - val_loss: 0.5222 - val_accuracy: 0.7500\nEpoch 85/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.3842 - accuracy: 0.8815\nEpoch 86/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.3709 - accuracy: 0.8870 - val_loss: 0.5276 - val_accuracy: 0.8000\nEpoch 87/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.3651 - accuracy: 0.8907\nEpoch 88/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.3748 - accuracy: 0.8806 - val_loss: 0.5085 - val_accuracy: 0.8000\nEpoch 89/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.3611 - accuracy: 0.8815\nEpoch 90/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.3604 - accuracy: 0.8907 - val_loss: 0.4910 - val_accuracy: 0.8083\nEpoch 91/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.3523 - accuracy: 0.8972\nEpoch 92/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.3530 - accuracy: 0.8935 - val_loss: 0.5018 - val_accuracy: 0.8000\nEpoch 93/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.3487 - accuracy: 0.8935\nEpoch 94/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.3433 - accuracy: 0.8954 - val_loss: 0.5011 - val_accuracy: 0.8083\nEpoch 95/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.3470 - accuracy: 0.8935\nEpoch 96/100\n17/17 [==============================] - 0s 14ms/step - loss: 0.3492 - accuracy: 0.8935 - val_loss: 0.5071 - val_accuracy: 0.8000\nEpoch 97/100\n17/17 [==============================] - 0s 12ms/step - loss: 0.3434 - accuracy: 0.9009\nEpoch 98/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.3376 - accuracy: 0.9000 - val_loss: 0.5033 - val_accuracy: 0.8000\nEpoch 99/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.3425 - accuracy: 0.8907\nEpoch 100/100\n17/17 [==============================] - 0s 13ms/step - loss: 0.3363 - accuracy: 0.8889 - val_loss: 0.4881 - val_accuracy: 0.8000\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3ab818fe50>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "model.fit(train_db, epochs=100, validation_data=test_db, validation_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Course 2, you had built a fully-connected network for this dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it.\n",
    "\n",
    "To get started, let's examine the shapes of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "number of training examples = 1080\nnumber of test examples = 120\nX_train shape: (1080, 64, 64, 3)\nY_train shape: (1080, 6)\nX_test shape: (120, 64, 64, 3)\nY_test shape: (120, 6)\n"
    }
   ],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "conv_layers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1 - Create placeholders\n",
    "\n",
    "TensorFlow requires that you create placeholders for the input data that will be fed into the model when running the session.\n",
    "\n",
    "**Exercise**: Implement the function below to create placeholders for the input image X and the output Y. You should not define the number of training examples for the moment. To do so, you could use \"None\" as the batch size, it will give you the flexibility to choose it later. Hence X should be of dimension **[None, n_H0, n_W0, n_C0]** and Y should be of dimension **[None, n_y]**.  [Hint](https://www.tensorflow.org/api_docs/python/tf/placeholder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_placeholders\n",
    "\n",
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_H0 -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (2 lines)\n",
    "    X = tf.placeholder(tf.float32, [None, n_H0, n_W0, n_C0])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_y])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b900d089d9ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"X = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Y = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-441f81f64ea1>\u001b[0m in \u001b[0;36mcreate_placeholders\u001b[0;34m(n_H0, n_W0, n_C0, n_y)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m### START CODE HERE ### (2 lines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_H0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_W0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_C0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(64, 64, 3, 6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "<td>\n",
    "    X = Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    Y = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Initialize parameters\n",
    "\n",
    "You will initialize weights/filters $W1$ and $W2$ using `tf.contrib.layers.xavier_initializer(seed = 0)`. You don't need to worry about bias variables as you will soon see that TensorFlow functions take care of the bias. Note also that you will only initialize the weights/filters for the conv2d functions. TensorFlow initializes the layers for the fully connected part automatically. We will talk more about that later in this assignment.\n",
    "\n",
    "**Exercise:** Implement initialize_parameters(). The dimensions for each group of filters are provided below. Reminder - to initialize a parameter $W$ of shape [1,2,3,4] in Tensorflow, use:\n",
    "```python\n",
    "W = tf.get_variable(\"W\", [1,2,3,4], initializer = ...)\n",
    "```\n",
    "[More Info](https://www.tensorflow.org/api_docs/python/tf/get_variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]\n",
    "                        W2 : [2, 2, 8, 16]\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                              # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 2 lines of code)\n",
    "    W1 = tf.get_variable(\"W1\", [4, 4, 3, 8], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W2 = tf.get_variable(\"W2\", [2, 2, 8, 16], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394\n",
      " -0.06847463  0.05245192]\n",
      "W2 = [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n",
      " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n",
      " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"].eval()[1,1,1]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"].eval()[1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output:**\n",
    "\n",
    "<table> \n",
    "\n",
    "    <tr>\n",
    "        <td>\n",
    "        W1 = \n",
    "        </td>\n",
    "        <td>\n",
    "[ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394 <br>\n",
    " -0.06847463  0.05245192]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "    <tr>\n",
    "        <td>\n",
    "        W2 = \n",
    "        </td>\n",
    "        <td>\n",
    "[-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058 <br>\n",
    " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228 <br>\n",
    " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Forward propagation\n",
    "\n",
    "In TensorFlow, there are built-in functions that carry out the convolution steps for you.\n",
    "\n",
    "- **tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = 'SAME'):** given an input $X$ and a group of filters $W1$, this function convolves $W1$'s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation [here](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
    "\n",
    "- **tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME'):** given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. You can read the full documentation [here](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool)\n",
    "\n",
    "- **tf.nn.relu(Z1):** computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n",
    "\n",
    "- **tf.contrib.layers.flatten(P)**: given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. It returns a flattened tensor with shape [batch_size, k]. You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten)\n",
    "\n",
    "- **tf.contrib.layers.fully_connected(F, num_outputs):** given a the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected)\n",
    "\n",
    "In the last function above (`tf.contrib.layers.fully_connected`), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters. \n",
    "\n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "Implement the `forward_propagation` function below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED`. You should use the functions above. \n",
    "\n",
    "In detail, we will use the following parameters for all the steps:\n",
    "     - Conv2D: stride 1, padding is \"SAME\"\n",
    "     - ReLU\n",
    "     - Max pool: Use an 8 by 8 filter size and an 8 by 8 stride, padding is \"SAME\"\n",
    "     - Conv2D: stride 1, padding is \"SAME\"\n",
    "     - ReLU\n",
    "     - Max pool: Use a 4 by 4 filter size and a 4 by 4 stride, padding is \"SAME\"\n",
    "     - Flatten the previous output.\n",
    "     - FULLYCONNECTED (FC) layer: Apply a fully connected layer without an non-linear activation function. Do not call the softmax here. This will result in 6 neurons in the output layer, which then get passed later to a softmax. In TensorFlow, the softmax and cost function are lumped together into a single function, which you'll call in a different function when computing the cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # CONV2D: stride of 1, padding 'SAME'\n",
    "    Z1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    # MAXPOOL: window 8x8, stride 8, padding 'SAME'\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1, 8, 8, 1], strides = [1, 8, 8, 1], padding='SAME')\n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    Z2 = tf.nn.conv2d(P1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1, 4, 4, 1], strides = [1, 4, 4, 1], padding='SAME')\n",
    "    # FLATTEN\n",
    "    P = tf.contrib.layers.flatten(P2)\n",
    "    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n",
    "    Z3 = tf.contrib.layers.fully_connected(P, 6, activation_fn=None)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z3 = [[-0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376  0.46852064]\n",
      " [-0.17601591 -1.57972014 -1.4737016  -2.61672091 -1.00810647  0.5747785 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholders(64, 64, 3, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a = sess.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\n",
    "    print(\"Z3 = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table> \n",
    "    <td> \n",
    "    Z3 =\n",
    "    </td>\n",
    "    <td>\n",
    "    [[-0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376  0.46852064] <br>\n",
    " [-0.17601591 -1.57972014 -1.4737016  -2.61672091 -1.00810647  0.5747785 ]]\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Compute cost\n",
    "\n",
    "Implement the compute cost function below. You might find these two functions helpful: \n",
    "\n",
    "- **tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):** computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation  [here.](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "- **tf.reduce_mean:** computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)\n",
    "\n",
    "** Exercise**: Compute the cost below using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 2.91034\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholders(64, 64, 3, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a = sess.run(cost, {X: np.random.randn(4,64,64,3), Y: np.random.randn(4,6)})\n",
    "    print(\"cost = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "    <td> \n",
    "    cost =\n",
    "    </td> \n",
    "    \n",
    "    <td> \n",
    "    2.91034\n",
    "    </td> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Model \n",
    "\n",
    "Finally you will merge the helper functions you implemented above to build a model. You will train it on the SIGNS dataset. \n",
    "\n",
    "You have implemented `random_mini_batches()` in the Optimization programming assignment of course 2. Remember that this function returns a list of mini-batches. \n",
    "\n",
    "**Exercise**: Complete the function below. \n",
    "\n",
    "The model below should:\n",
    "\n",
    "- create placeholders\n",
    "- initialize parameters\n",
    "- forward propagate\n",
    "- compute the cost\n",
    "- create an optimizer\n",
    "\n",
    "Finally you will create a session and run a for loop  for num_epochs, get the mini-batches, and then for each mini-batch you will optimize the function. [Hint for initializing the variables](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate=0.009,\n",
    "          num_epochs=100, minibatch_size=64, print_cost=True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer ConvNet in Tensorflow:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 3                                          # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = initialize_parameters()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model for 100 epochs. Check if your cost after epoch 0 and 5 matches our output. If not, stop the cell and go back to your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.917929\n",
      "Cost after epoch 5: 1.506757\n",
      "Cost after epoch 10: 0.955359\n",
      "Cost after epoch 15: 0.845802\n",
      "Cost after epoch 20: 0.701174\n",
      "Cost after epoch 25: 0.571977\n",
      "Cost after epoch 30: 0.518435\n",
      "Cost after epoch 35: 0.495806\n",
      "Cost after epoch 40: 0.429827\n",
      "Cost after epoch 45: 0.407291\n",
      "Cost after epoch 50: 0.366394\n",
      "Cost after epoch 55: 0.376922\n",
      "Cost after epoch 60: 0.299491\n",
      "Cost after epoch 65: 0.338870\n",
      "Cost after epoch 70: 0.316400\n",
      "Cost after epoch 75: 0.310413\n",
      "Cost after epoch 80: 0.249549\n",
      "Cost after epoch 85: 0.243457\n",
      "Cost after epoch 90: 0.200031\n",
      "Cost after epoch 95: 0.175452\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPypHYdmk//r2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa860fcdc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
      "Train Accuracy: 0.940741\n",
      "Test Accuracy: 0.783333\n"
     ]
    }
   ],
   "source": [
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**: although it may not match perfectly, your expected output should be close to ours and your cost value should decrease.\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "    <td> \n",
    "    **Cost after epoch 0 =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      1.917929\n",
    "    </td> \n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **Cost after epoch 5 =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      1.506757\n",
    "    </td> \n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **Train Accuracy   =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      0.940741\n",
    "    </td> \n",
    "</tr> \n",
    "\n",
    "<tr>\n",
    "    <td> \n",
    "    **Test Accuracy   =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      0.783333\n",
    "    </td> \n",
    "</tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have finised the assignment and built a model that recognizes SIGN language with almost 80% accuracy on the test set. If you wish, feel free to play around with this dataset further. You can actually improve its accuracy by spending more time tuning the hyperparameters, or using regularization (as this model clearly has a high variance). \n",
    "\n",
    "Once again, here's a thumbs up for your work! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa860df3588>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa860edbfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fname = \"images/thumbs_up.jpg\"\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "my_image = scipy.misc.imresize(image, size=(64,64))\n",
    "plt.imshow(my_image)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "bwbJV",
   "launcher_item_id": "0TkXB"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}